{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 740M\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import theano\n",
    "import theano.tensor.signal.downsample\n",
    "from common.plotting import plot_mat\n",
    "from utils import *\n",
    "import time\n",
    "\n",
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing ('features', 'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "from prepare_cifar10 import *\n",
    "\n",
    "cifar = prepare_cifar10()\n",
    "\n",
    "cifar_train = cifar.train\n",
    "cifar_train_stream = cifar.train_stream\n",
    "                                               \n",
    "cifar_validation = cifar.validation\n",
    "cifar_validation_stream = cifar.validation_stream\n",
    "\n",
    "cifar_test = cifar.test\n",
    "cifar_test_stream = cifar.test_stream\n",
    "\n",
    "print(\"The streams return batches containing %s\" % (cifar_train_stream.sources,))\n",
    "\n",
    "print(\"Each trainin batch consits of a tuple containing:\")\n",
    "for element in next(cifar_train_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))\n",
    "    \n",
    "print(\"Validation/test batches consits of tuples containing:\")\n",
    "for element in next(cifar_test_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def conv2D(num_in_filters, num_out_filters, kernel_size, name = None):\n",
    "    name = name if name else fresh_name(\"?\")\n",
    "        \n",
    "    weights_shape = (num_out_filters, num_in_filters, kernel_size, kernel_size)\n",
    "    weights_name = \"{}.weights\".format(name)\n",
    "    weights = theano.shared(np.zeros(weights_shape, dtype = 'float32'), name = weights_name)\n",
    "    weights.tag.initializer = IsotropicGaussian(0.05)\n",
    "    \n",
    "    biases_shape = (num_out_filters,)\n",
    "    biases_name = \"{}.biases\".format(name)\n",
    "    biases = theano.shared(np.zeros(biases_shape, dtype='float32'), biases_name)\n",
    "    biases.tag.initializer = Constant(0.0)\n",
    "    \n",
    "    def fprop(X):\n",
    "        return theano.tensor.nnet.conv2d(X, weights) + biases.dimshuffle('x', 0, 'x', 'x')\n",
    "    \n",
    "    fprop.params = [weights, biases]\n",
    "    \n",
    "    return fprop\n",
    "\n",
    "def relu(name = fresh_name(\"?\")):\n",
    "    def fprop(X):\n",
    "        return theano.tensor.maximum(0.0, X)\n",
    "    \n",
    "    return fprop\n",
    "    \n",
    "def max_pool_2d(kernel_size):\n",
    "    def fprop(X):\n",
    "        kernel_shape = (kernel_size, kernel_size)\n",
    "        return theano.tensor.signal.downsample.max_pool_2d(X, kernel_shape, ignore_border = True)\n",
    "\n",
    "    return fprop\n",
    "\n",
    "def flatten(num_inputs, num_outputs, name = None):\n",
    "    name = name if name else fresh_name(\"?\")\n",
    "        \n",
    "    weights_shape = (num_inputs, num_outputs)\n",
    "    weights_name = \"{}.weights\".format(name)\n",
    "    weights = theano.shared(np.zeros(weights_shape, dtype='float32'), name = weights_name)\n",
    "    weights.tag.initializer = IsotropicGaussian(0.05)\n",
    "    \n",
    "    biases_shape = (num_outputs, )\n",
    "    biases_name = \"{}.biases\".format(name)\n",
    "    biases = theano.shared(np.zeros(biases_shape, dtype='float32'), name = biases_name)\n",
    "    biases.tag.initializer = Constant(0.0)\n",
    "    \n",
    "    def fprop(X):\n",
    "        return theano.tensor.dot(X.flatten(2), weights) + biases.dimshuffle('x', 0)\n",
    "\n",
    "    fprop.params = [weights, biases]\n",
    "    \n",
    "    return fprop\n",
    "    \n",
    "def xaffine(num_inputs, num_outputs, name = None):    \n",
    "    name = name if name else fresh_name(\"?\")\n",
    "    \n",
    "    weights_shape = (num_inputs, num_outputs)\n",
    "    weights_name = \"{}.weights\".format(name)\n",
    "    weights = theano.shared(np.zeros(weights_shape, dtype='float32'), name = weights_name)\n",
    "    weights.tag.initializer = IsotropicGaussian(0.05)\n",
    "    \n",
    "    biases_shape = (num_outputs, )\n",
    "    biases_name = \"{}.biases\".format(name)\n",
    "    biases = theano.shared(np.zeros(biases_shape, dtype='float32'), name = biases_name)\n",
    "    biases.tag.initializer = Constant(0.0)\n",
    "    \n",
    "    def fprop(X):\n",
    "        return theano.tensor.dot(X, weights) + biases.dimshuffle('x', 0)\n",
    "\n",
    "    fprop.params = [weights, biases]\n",
    "    \n",
    "    return fprop\n",
    "    \n",
    "def maxout(num_inputs, num_outputs, degree, name = None):\n",
    "    name = name if name else fresh_name(\"?\")\n",
    "    \n",
    "    weights_shape = (num_outputs, num_inputs, degree)\n",
    "    weights_name = \"{}.weights\".format(name)\n",
    "    weights = theano.shared(np.zeros(weights_shape, dtype='float32'), name = weights_name)\n",
    "    weights.tag.initializer = IsotropicGaussian(0.05)\n",
    "    \n",
    "    biases_shape = (num_outputs, degree)\n",
    "    biases_name = \"{}.biases\".format(name)\n",
    "    biases = theano.shared(np.zeros(biases_shape, dtype='float32'), name = biases_name)\n",
    "    biases.tag.initializer = Constant(0.0)\n",
    "    \n",
    "    def fprop(X):\n",
    "        return theano.tensor.max(theano.tensor.dot(X, weights) + biases.dimshuffle('x', 0, 1), axis = 2)\n",
    "    \n",
    "    fprop.params = [weights, biases]\n",
    "    \n",
    "    return fprop\n",
    "    \n",
    "def softmax():\n",
    "    def fprop(X):\n",
    "        return theano.tensor.nnet.softmax(X)\n",
    "\n",
    "    return fprop\n",
    "    \n",
    "def compose(*args):\n",
    "    def fprop(X):\n",
    "        for arg in args:\n",
    "            X = arg(X)\n",
    "        return X\n",
    "    \n",
    "    params = []\n",
    "\n",
    "    for arg in args:\n",
    "        try:\n",
    "            params += arg.params\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    if params != []:\n",
    "        fprop.params = params\n",
    "    \n",
    "    return fprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile(template):\n",
    "    X = theano.tensor.tensor4('X', dtype = 'float32')\n",
    "    Y = theano.tensor.matrix('Y', dtype = 'uint8')\n",
    "    \n",
    "    model_parameters = template.params\n",
    "\n",
    "    log_probs = template(X)\n",
    "\n",
    "    predictions = theano.tensor.argmax(log_probs, axis = 1)\n",
    "\n",
    "    error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "    nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "    weight_decay = 0.0\n",
    "    for p in model_parameters:\n",
    "        if p.name.endswith('weights'):\n",
    "            weight_decay = weight_decay + 1e-3 * (p ** 2).sum()\n",
    "\n",
    "    cost = nll + weight_decay\n",
    "    \n",
    "    learn_rate = theano.tensor.scalar('learn_rate', dtype = 'float32')\n",
    "    momentum = theano.tensor.scalar('momentum', dtype = 'float32')\n",
    "\n",
    "    # Theano will compute the gradients for us\n",
    "    gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "    #initialize storage for momentum\n",
    "    velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in model_parameters]\n",
    "    \n",
    "    updates = []\n",
    "\n",
    "    for p, g, v in zip(model_parameters, gradients, velocities):\n",
    "        if p.name.endswith(\"weights\"):\n",
    "            g += 1e-3 * p\n",
    "        v_new = momentum * v - learn_rate * g\n",
    "        p_new = p + v_new\n",
    "        updates += [(v, v_new), (p, p_new)]\n",
    "            \n",
    "    def step_ex(X, Y, learn_rate, momentum):\n",
    "        return step(X, Y, learn_rate, momentum)\n",
    "    \n",
    "    def init_parameters():\n",
    "        rng = numpy.random.RandomState(1234)\n",
    "        \n",
    "        for p in model_parameters:\n",
    "            p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "            \n",
    "        for v in velocities:\n",
    "            v.set_value(np.zeros_like(v.get_value()))\n",
    "            \n",
    "    step = theano.function(\n",
    "        [X, Y, learn_rate, momentum],\n",
    "        [cost, error_rate, nll, weight_decay],\n",
    "        updates = updates, \n",
    "        allow_input_downcast = True)\n",
    "            \n",
    "    predict = theano.function([X], predictions)\n",
    "    \n",
    "    init_parameters()\n",
    "    \n",
    "    class Network:\n",
    "        def snapshot(self):\n",
    "            return [p.get_value(borrow = False) for p in self.params]\n",
    "    \n",
    "        def load(self, snapshot):\n",
    "            for p, s in zip(self.params, snapshot):\n",
    "                p.set_value(s, borrow = False)\n",
    "    \n",
    "    network = Network()\n",
    "    network.step = step_ex\n",
    "    network.predict = predict\n",
    "    network.params = model_parameters\n",
    "\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(network, learn_rate0, momentum):\n",
    "    batch = 0\n",
    "    epoch = 0\n",
    "\n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = network.snapshot()\n",
    "    best_params_epoch = 0\n",
    "\n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    train_nll = []\n",
    "    validation_errors = []\n",
    "\n",
    "    number_of_epochs = 3\n",
    "    patience_expansion = 1.5\n",
    "\n",
    "    # training loop\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        while epoch < number_of_epochs: #This loop goes over epochs\n",
    "            epoch += 1\n",
    "            #First train on all data from this batch\n",
    "\n",
    "            epoch_start_batch = batch\n",
    "\n",
    "            for X_batch, Y_batch in cifar_train_stream.get_epoch_iterator(): \n",
    "                batch += 1\n",
    "\n",
    "                # learn_rate = learn_rate0 * (1 - np.tanh(batch * 0.549306 / 2000))\n",
    "                \n",
    "                K = 2000\n",
    "                learn_rate = learn_rate0 * K / np.maximum(K, batch)\n",
    "                \n",
    "                L, err_rate, nll, wdec = network.step(X_batch, Y_batch, learn_rate, momentum)\n",
    "\n",
    "                train_loss.append((batch, L))\n",
    "                train_erros.append((batch, err_rate))\n",
    "                train_nll.append((batch, nll))\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    end = time.time()\n",
    "                    elapsed = end - start\n",
    "                    start = end\n",
    "                    format = \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%, time %.2fms\"\n",
    "                    print(format % (batch, L, nll, err_rate * 100, elapsed / Y_batch.shape[0] * 1000), flush = True)\n",
    "\n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(cifar_validation_stream, network.predict)\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, epoch * patience_expansion + 1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = network.snapshot()\n",
    "                best_params_epoch = epoch\n",
    "            validation_errors.append((batch, val_error_rate))\n",
    "\n",
    "            print(\"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "                epoch, val_error_rate * 100, number_of_epochs))\n",
    "            print(\"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "                epoch, np.mean(np.asarray(train_erros)[epoch_start_batch:, 1]) * 100, \n",
    "                np.mean(np.asarray(train_nll)[epoch_start_batch:, 1]),\n",
    "                np.mean(np.asarray(train_loss)[epoch_start_batch:, 1])))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Setting network parameters from after epoch %d\" %(best_params_epoch))\n",
    "        network.load(best_params)\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_nll_a = np.array(train_nll)\n",
    "        semilogy(train_nll_a[:,0], train_nll_a[:,1], label='batch train nll')\n",
    "        legend()\n",
    "\n",
    "        subplot(2,1,2)\n",
    "        train_erros_a = np.array(train_erros)\n",
    "        plot(train_erros_a[:,0], train_erros_a[:,1], label='batch train error rate')\n",
    "        validation_errors_a = np.array(validation_errors)\n",
    "        plot(validation_errors_a[:,0], validation_errors_a[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/wojciech/.theano/compiledir_Linux-3.19--generic-x86_64-with-LinuxMint-17.3-rosa-x86_64-3.4.3-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /home/wojciech/.theano/compiledir_Linux-3.19--generic-x86_64-with-LinuxMint-17.3-rosa-x86_64-3.4.3-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "At minibatch 100, batch loss 2.747560, batch nll 1.966893, batch error rate 68.000000%, time 255.62ms\n"
     ]
    }
   ],
   "source": [
    "nn = compose(conv2D(3, 64, 5), \n",
    "             relu(), \n",
    "             max_pool_2d(2),\n",
    "             conv2D(64, 25, 5),\n",
    "             relu(),\n",
    "             max_pool_2d(2),\n",
    "             flatten(625, 200),\n",
    "             relu(),\n",
    "             xaffine(200, 10), \n",
    "             softmax()\n",
    "            )\n",
    "\n",
    "nn = compose(conv2D(3, 64, 5), \n",
    "             relu(), \n",
    "             max_pool_2d(2),\n",
    "             conv2D(64, 64, 5),\n",
    "             relu(),\n",
    "             max_pool_2d(2),\n",
    "             flatten(1600, 128),\n",
    "             relu(),\n",
    "             maxout(128, 10, 4),\n",
    "             softmax()\n",
    "            )\n",
    "\n",
    "print(\"Compiling...\", end = \" \", flush = True)\n",
    "network = compile(nn)\n",
    "print(\"DONE\", flush = True)\n",
    "\n",
    "train(network, 4e-3, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error rate is 28.640000%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Test error rate is %f%%\" %(compute_error_rate(cifar_test_stream, network.predict) * 100.0,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAC/CAYAAAA1gI2yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDQAOoWkDgENo2gDgkP8D+IrcmcNVB1IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ae6054ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#How do the filters in the first layer look like?\n",
    "\n",
    "for param in network.params:\n",
    "    if param.name.endswith('weights'):\n",
    "        plot_mat(param.get_value(), cmap='gray')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iii = network.predict.maker.inputs[0]\n",
    "X = iii.variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'after_C1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-664c4f416549>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m middle_layers_computer = theano.function([X], [\n\u001b[0;32m      4\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mafter_C1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mafter_P1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mafter_C2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'after_C1' is not defined"
     ]
    }
   ],
   "source": [
    "# build a function that shows how the network processes an image\n",
    "\n",
    "middle_layers_computer = theano.function([X], [\n",
    "        X,\n",
    "        after_C1,\n",
    "        after_P1,\n",
    "        after_C2,\n",
    "        after_P2\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_num=4\n",
    "\n",
    "middle_layers = middle_layers_computer(X_test_value[img_num:img_num+1])\n",
    "\n",
    "for ml, name in zip(middle_layers, ['X', 'C1', 'P1', 'C2', 'P2']):\n",
    "    plot_mat(ml.transpose(1,0,2,3), cmap='gray')\n",
    "    title(name)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
