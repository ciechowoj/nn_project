{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 740M\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import theano\n",
    "import theano.tensor.signal.downsample\n",
    "from common.plotting import plot_mat\n",
    "\n",
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing ('features', 'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "from prepare_cifar10 import *\n",
    "\n",
    "cifar = prepare_cifar10()\n",
    "\n",
    "cifar_train = cifar.train\n",
    "cifar_train_stream = cifar.train_stream\n",
    "                                               \n",
    "cifar_validation = cifar.validation\n",
    "cifar_validation_stream = cifar.validation_stream\n",
    "\n",
    "cifar_test = cifar.test\n",
    "cifar_test_stream = cifar.test_stream\n",
    "\n",
    "print(\"The streams return batches containing %s\" % (cifar_train_stream.sources,))\n",
    "\n",
    "print(\"Each trainin batch consits of a tuple containing:\")\n",
    "for element in next(cifar_train_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))\n",
    "    \n",
    "print(\"Validation/test batches consits of tuples containing:\")\n",
    "for element in next(cifar_test_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = theano.tensor.tensor4('X', dtype='float32') # (batch_size x num_channels x img_rows x img_columns)\n",
    "\n",
    "Y = theano.tensor.matrix('Y', dtype='uint8') # Y is 1D, it lists the targets for all examples\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "\n",
    "\n",
    "#The first convolutional layer\n",
    "#The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = 10 #we will apply that many convolution filters in the first layer\n",
    "CW1 = theano.shared(np.zeros((num_filters_1, 3, 5, 5), dtype = 'float32'),\n",
    "                   name='CW1')\n",
    "#please note - this is somewhat non-standard\n",
    "CW1.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype='float32'),\n",
    "                    name='CB1')\n",
    "\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "\n",
    "\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2,2), ignore_border=True)\n",
    "\n",
    "num_filters_2 = 25 #we will compute ten convolution filters in the first layer\n",
    "CW2 = theano.shared(np.zeros((num_filters_2,num_filters_1,5,5), dtype='float32'),\n",
    "                   name='CW2')\n",
    "\n",
    "CW2.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype='float32'),\n",
    "                    name='CB2')\n",
    "\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2,2), ignore_border=True)\n",
    "\n",
    "#Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden=500\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 5 * 5, num_fw3_hidden), dtype='float32'),\n",
    "                   name='FW3')\n",
    "FW3.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype='float32'),\n",
    "                    name='FB3')\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle('x',0))\n",
    "# print \"after_F3 shape: %s\" % (after_F3.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_fw4_hidden=10\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype='float32'),\n",
    "                   name='FW4')\n",
    "FW4.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype='float32'),\n",
    "                    name='FB4')\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle('x',0)\n",
    "# print \"after_F4 shape: %s\" % (after_F4.tag.test_value.shape,)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis=1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1]=='W':\n",
    "        weight_decay = weight_decay + 1e-3 * (p**2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def conv2D(num_in_filters, num_out_filters, kernel_size, name = None):\n",
    "    name = name if name else fresh_name(\"?\")\n",
    "        \n",
    "    weights_shape = (num_out_filters, num_in_filters, kernel_size, kernel_size)\n",
    "    weights_name = \"{}.weights\".format(name)\n",
    "    weights = theano.shared(np.zeros(weights_shape, dtype = 'float32'), name = weights_name)\n",
    "    weights.tag.initializer = IsotropicGaussian(0.05)\n",
    "    \n",
    "    biases_shape = (num_out_filters,)\n",
    "    biases_name = \"{}.biases\".format(name)\n",
    "    biases = theano.shared(np.zeros(biases_shape, dtype='float32'), biases_name)\n",
    "    biases.tag.initializer = Constant(0.0)\n",
    "    \n",
    "    def fprop(X):\n",
    "        return theano.tensor.nnet.conv2d(X, weights) + biases.dimshuffle('x', 0, 'x', 'x')\n",
    "    \n",
    "    fprop.params = [weights, biases]\n",
    "    \n",
    "    return fprop\n",
    "\n",
    "def relu(name = fresh_name(\"?\")):\n",
    "    def fprop(X):\n",
    "        return theano.tensor.maximum(0.0, X)\n",
    "    \n",
    "    return fprop\n",
    "    \n",
    "def max_pool_2d(kernel_size):\n",
    "    def fprop(X):\n",
    "        kernel_shape = (kernel_size, kernel_size)\n",
    "        return theano.tensor.signal.downsample.max_pool_2d(X, kernel_shape, ignore_border = True)\n",
    "\n",
    "    return fprop\n",
    "\n",
    "def flatten(num_inputs, num_outputs, name = None):\n",
    "    name = name if name else fresh_name(\"?\")\n",
    "        \n",
    "    weights_shape = (num_inputs, num_outputs)\n",
    "    weights_name = \"{}.weights\".format(name)\n",
    "    weights = theano.shared(np.zeros(weights_shape, dtype='float32'), name = weights_name)\n",
    "    weights.tag.initializer = IsotropicGaussian(0.05)\n",
    "    \n",
    "    biases_shape = (num_outputs, )\n",
    "    biases_name = \"{}.biases\".format(name)\n",
    "    biases = theano.shared(np.zeros(biases_shape, dtype='float32'), name = biases_name)\n",
    "    biases.tag.initializer = Constant(0.0)\n",
    "    \n",
    "    def fprop(X):\n",
    "        return theano.tensor.dot(X.flatten(2), weights) + biases.dimshuffle('x', 0)\n",
    "\n",
    "    fprop.params = [weights, biases]\n",
    "    \n",
    "    return fprop\n",
    "    \n",
    "def xaffine(num_inputs, num_outputs, name = None):    \n",
    "    name = name if name else fresh_name(\"?\")\n",
    "    \n",
    "    weights_shape = (num_inputs, num_outputs)\n",
    "    weights_name = \"{}.weights\".format(name)\n",
    "    weights = theano.shared(np.zeros(weights_shape, dtype='float32'), name = weights_name)\n",
    "    weights.tag.initializer = IsotropicGaussian(0.05)\n",
    "    \n",
    "    biases_shape = (num_outputs, )\n",
    "    biases_name = \"{}.biases\".format(name)\n",
    "    biases = theano.shared(np.zeros(biases_shape, dtype='float32'), name = biases_name)\n",
    "    biases.tag.initializer = Constant(0.0)\n",
    "    \n",
    "    def fprop(X):\n",
    "        return theano.tensor.dot(X, weights) + biases.dimshuffle('x', 0)\n",
    "\n",
    "    fprop.params = [weights, biases]\n",
    "    \n",
    "    return fprop\n",
    "    \n",
    "def softmax():\n",
    "    def fprop(X):\n",
    "        return theano.tensor.nnet.softmax(X)\n",
    "\n",
    "    return fprop\n",
    "    \n",
    "def compose(*args):\n",
    "    def fprop(X):\n",
    "        for arg in args:\n",
    "            X = arg(X)\n",
    "        return X\n",
    "    \n",
    "    params = []\n",
    "\n",
    "    for arg in args:\n",
    "        try:\n",
    "            params += arg.params\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    if params != []:\n",
    "        fprop.params = params\n",
    "    \n",
    "    return fprop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X = theano.tensor.tensor4('X', dtype='float32') # (batch_size x num_channels x img_rows x img_columns)\n",
    "\n",
    "Y = theano.tensor.matrix('Y', dtype='uint8') # Y is 1D, it lists the targets for all examples\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "nn = compose(conv2D(3, 30, 5), \n",
    "             relu(), \n",
    "             max_pool_2d(2),\n",
    "             conv2D(30, 25, 5),\n",
    "             relu(),\n",
    "             max_pool_2d(2),\n",
    "             flatten(25 * 5 * 5, 200),\n",
    "             relu(),\n",
    "             xaffine(200, 10)\n",
    "            )\n",
    "\n",
    "model_parameters = nn.params\n",
    "\n",
    "after_F4 = nn(X)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis=1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name.endswith('weights'):\n",
    "        weight_decay = weight_decay + 1e-3 * (p**2).sum()\n",
    "\n",
    "cost = nll + weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar('lrate',dtype='float32')\n",
    "momentum = theano.tensor.scalar('momentum',dtype='float32')\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in model_parameters]\n",
    "\n",
    "for p,g,v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v,v_new), (p, p_new)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_?141.weights, Elemwise{sub,no_inplace}.0),\n",
       " (?141.weights, Elemwise{add,no_inplace}.0),\n",
       " (V_?141.biases, Elemwise{sub,no_inplace}.0),\n",
       " (?141.biases, Elemwise{add,no_inplace}.0),\n",
       " (V_?142.weights, Elemwise{sub,no_inplace}.0),\n",
       " (?142.weights, Elemwise{add,no_inplace}.0),\n",
       " (V_?142.biases, Elemwise{sub,no_inplace}.0),\n",
       " (?142.biases, Elemwise{add,no_inplace}.0),\n",
       " (V_?143.weights, Elemwise{sub,no_inplace}.0),\n",
       " (?143.weights, Elemwise{add,no_inplace}.0),\n",
       " (V_?143.biases, Elemwise{sub,no_inplace}.0),\n",
       " (?143.biases, Elemwise{add,no_inplace}.0),\n",
       " (V_?144.weights, Elemwise{sub,no_inplace}.0),\n",
       " (?144.weights, Elemwise{add,no_inplace}.0),\n",
       " (V_?144.biases, Elemwise{sub,no_inplace}.0),\n",
       " (?144.biases, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile theano functions\n",
    "\n",
    "#each call to train step will make one SGD step\n",
    "train_step = theano.function([X,Y,lrate,momentum],[cost, error_rate, nll, weight_decay],updates=updates, allow_input_downcast=True)\n",
    "#each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile(network, learning_rate, momentum):    \n",
    "    X = theano.tensor.tensor4('X', dtype='float32')\n",
    "    Y = theano.tensor.matrix('Y', dtype='uint8')\n",
    "\n",
    "    model_params = network.params\n",
    "\n",
    "model_parameters = nn.params\n",
    "\n",
    "after_F4 = nn(X)\n",
    "\n",
    "log_probs = netwo\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis=1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name.endswith('weights'):\n",
    "        weight_decay = weight_decay + 1e-3 * (p**2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X)!=Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs/num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utilities to save values of parameters and to load them\n",
    "\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow=False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "\n",
    "i=0\n",
    "e=0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.385125, batch nll 2.019164, batch error rate 78.000000%\n",
      "At minibatch 200, batch loss 2.111337, batch nll 1.748627, batch error rate 70.000000%\n",
      "At minibatch 300, batch loss 1.929005, batch nll 1.569374, batch error rate 55.000000%\n",
      "At minibatch 400, batch loss 1.891762, batch nll 1.535017, batch error rate 52.000000%\n",
      "After epoch 1: valid_err_rate: 55.990000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 67.207500% averaged train nll: 1.856404 averaged train loss: 2.219271\n",
      "At minibatch 500, batch loss 2.010554, batch nll 1.657150, batch error rate 56.000000%\n",
      "At minibatch 600, batch loss 1.940486, batch nll 1.590555, batch error rate 57.000000%\n",
      "At minibatch 700, batch loss 1.785677, batch nll 1.439448, batch error rate 54.000000%\n",
      "At minibatch 800, batch loss 1.724261, batch nll 1.381467, batch error rate 48.000000%\n",
      "After epoch 2: valid_err_rate: 49.300000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 52.495000% averaged train nll: 1.455646 averaged train loss: 1.805482\n",
      "At minibatch 900, batch loss 1.632166, batch nll 1.292166, batch error rate 51.000000%\n",
      "At minibatch 1000, batch loss 1.504359, batch nll 1.167648, batch error rate 39.000000%\n",
      "At minibatch 1100, batch loss 1.608770, batch nll 1.275301, batch error rate 45.000000%\n",
      "At minibatch 1200, batch loss 1.684636, batch nll 1.354270, batch error rate 44.000000%\n",
      "After epoch 3: valid_err_rate: 45.930000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 47.060000% averaged train nll: 1.318242 averaged train loss: 1.654928\n",
      "At minibatch 1300, batch loss 1.595669, batch nll 1.267868, batch error rate 46.000000%\n",
      "At minibatch 1400, batch loss 1.628881, batch nll 1.303958, batch error rate 49.000000%\n",
      "At minibatch 1500, batch loss 1.372309, batch nll 1.049801, batch error rate 38.000000%\n",
      "At minibatch 1600, batch loss 1.500800, batch nll 1.181105, batch error rate 39.000000%\n",
      "After epoch 4: valid_err_rate: 41.930000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 43.177500% averaged train nll: 1.218673 averaged train loss: 1.543758\n",
      "At minibatch 1700, batch loss 1.401600, batch nll 1.084144, batch error rate 37.000000%\n",
      "At minibatch 1800, batch loss 1.426364, batch nll 1.111216, batch error rate 35.000000%\n",
      "At minibatch 1900, batch loss 1.236073, batch nll 0.923255, batch error rate 37.000000%\n",
      "At minibatch 2000, batch loss 1.472811, batch nll 1.162427, batch error rate 39.000000%\n",
      "After epoch 5: valid_err_rate: 39.590000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 39.842500% averaged train nll: 1.126781 averaged train loss: 1.441907\n",
      "At minibatch 2100, batch loss 1.299000, batch nll 0.990796, batch error rate 39.000000%\n",
      "At minibatch 2200, batch loss 1.466498, batch nll 1.160643, batch error rate 41.000000%\n",
      "At minibatch 2300, batch loss 1.269427, batch nll 0.965428, batch error rate 31.000000%\n",
      "At minibatch 2400, batch loss 1.273901, batch nll 0.971728, batch error rate 37.000000%\n",
      "After epoch 6: valid_err_rate: 36.960000% currently going to do 10 epochs\n",
      "After epoch 6: averaged train_err_rate: 37.302500% averaged train nll: 1.055119 averaged train loss: 1.361158\n",
      "At minibatch 2500, batch loss 1.210666, batch nll 0.910132, batch error rate 34.000000%\n",
      "At minibatch 2600, batch loss 1.276332, batch nll 0.977607, batch error rate 38.000000%\n",
      "At minibatch 2700, batch loss 1.218977, batch nll 0.921945, batch error rate 30.000000%\n",
      "At minibatch 2800, batch loss 1.196918, batch nll 0.901376, batch error rate 32.000000%\n",
      "After epoch 7: valid_err_rate: 36.410000% currently going to do 11 epochs\n",
      "After epoch 7: averaged train_err_rate: 34.890000% averaged train nll: 1.000379 averaged train loss: 1.299181\n",
      "At minibatch 2900, batch loss 1.068603, batch nll 0.774184, batch error rate 29.000000%\n",
      "At minibatch 3000, batch loss 1.116206, batch nll 0.823283, batch error rate 28.000000%\n",
      "At minibatch 3100, batch loss 1.132981, batch nll 0.841114, batch error rate 25.000000%\n",
      "At minibatch 3200, batch loss 1.254224, batch nll 0.963895, batch error rate 36.000000%\n",
      "After epoch 8: valid_err_rate: 34.860000% currently going to do 13 epochs\n",
      "After epoch 8: averaged train_err_rate: 33.185000% averaged train nll: 0.948540 averaged train loss: 1.241539\n",
      "At minibatch 3300, batch loss 1.155000, batch nll 0.865607, batch error rate 28.000000%\n",
      "At minibatch 3400, batch loss 1.227787, batch nll 0.939403, batch error rate 32.000000%\n",
      "At minibatch 3500, batch loss 1.091256, batch nll 0.804033, batch error rate 24.000000%\n",
      "At minibatch 3600, batch loss 1.127371, batch nll 0.841275, batch error rate 25.000000%\n",
      "After epoch 9: valid_err_rate: 34.830000% currently going to do 14 epochs\n",
      "After epoch 9: averaged train_err_rate: 31.682500% averaged train nll: 0.907994 averaged train loss: 1.196271\n",
      "At minibatch 3700, batch loss 1.166717, batch nll 0.881434, batch error rate 37.000000%\n",
      "At minibatch 3800, batch loss 1.125044, batch nll 0.840508, batch error rate 36.000000%\n",
      "At minibatch 3900, batch loss 1.139607, batch nll 0.855953, batch error rate 24.000000%\n",
      "At minibatch 4000, batch loss 1.131168, batch nll 0.848436, batch error rate 30.000000%\n",
      "After epoch 10: valid_err_rate: 33.420000% currently going to do 16 epochs\n",
      "After epoch 10: averaged train_err_rate: 30.522500% averaged train nll: 0.880966 averaged train loss: 1.165438\n",
      "At minibatch 4100, batch loss 1.066710, batch nll 0.784639, batch error rate 31.000000%\n",
      "At minibatch 4200, batch loss 1.108152, batch nll 0.826832, batch error rate 29.000000%\n",
      "At minibatch 4300, batch loss 1.349521, batch nll 1.068960, batch error rate 37.000000%\n",
      "At minibatch 4400, batch loss 1.080820, batch nll 0.801038, batch error rate 25.000000%\n",
      "After epoch 11: valid_err_rate: 33.640000% currently going to do 16 epochs\n",
      "After epoch 11: averaged train_err_rate: 29.505000% averaged train nll: 0.847047 averaged train loss: 1.128325\n",
      "At minibatch 4500, batch loss 1.089842, batch nll 0.810665, batch error rate 28.000000%\n",
      "At minibatch 4600, batch loss 0.990965, batch nll 0.712342, batch error rate 26.000000%\n",
      "At minibatch 4700, batch loss 1.108261, batch nll 0.830153, batch error rate 25.000000%\n",
      "At minibatch 4800, batch loss 1.143795, batch nll 0.866484, batch error rate 33.000000%\n",
      "After epoch 12: valid_err_rate: 32.570000% currently going to do 19 epochs\n",
      "After epoch 12: averaged train_err_rate: 28.575000% averaged train nll: 0.825392 averaged train loss: 1.104015\n",
      "At minibatch 4900, batch loss 0.930788, batch nll 0.653903, batch error rate 25.000000%\n",
      "At minibatch 5000, batch loss 0.998544, batch nll 0.722101, batch error rate 27.000000%\n",
      "At minibatch 5100, batch loss 1.139214, batch nll 0.863352, batch error rate 29.000000%\n",
      "At minibatch 5200, batch loss 1.009145, batch nll 0.733799, batch error rate 25.000000%\n",
      "After epoch 13: valid_err_rate: 32.800000% currently going to do 19 epochs\n",
      "After epoch 13: averaged train_err_rate: 27.570000% averaged train nll: 0.800149 averaged train loss: 1.076558\n",
      "At minibatch 5300, batch loss 1.195934, batch nll 0.920965, batch error rate 36.000000%\n",
      "At minibatch 5400, batch loss 0.969079, batch nll 0.694508, batch error rate 25.000000%\n",
      "At minibatch 5500, batch loss 1.038424, batch nll 0.764432, batch error rate 28.000000%\n",
      "At minibatch 5600, batch loss 0.980554, batch nll 0.706994, batch error rate 27.000000%\n",
      "After epoch 14: valid_err_rate: 32.160000% currently going to do 22 epochs"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "while e < number_of_epochs: #This loop goes over epochs\n",
    "    e += 1\n",
    "    #First train on all data from this batch\n",
    "    \n",
    "    epoch_start_i = i\n",
    "    \n",
    "    for X_batch, Y_batch in cifar_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 4e-3 * K / np.maximum(K, i)\n",
    "        momentum=0.9\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        #print [p.get_value().ravel()[:10] for p in model_parameters]\n",
    "        #print [p.get_value().ravel()[:10] for p in velocities]\n",
    "        \n",
    "        \n",
    "        train_loss.append((i,L))\n",
    "        train_erros.append((i,err_rate))\n",
    "        train_nll.append((i,nll))\n",
    "        if i % 100 == 0:\n",
    "            print(\"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100))\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(cifar_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i,val_error_rate))\n",
    "    print(\"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "        e, val_error_rate*100, number_of_epochs))\n",
    "    print(\"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "        e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "        np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "        np.mean(np.asarray(train_loss)[epoch_start_i:,1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 29\n",
      "Test error rate is 32.060000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1d5b477860>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAECCAYAAAAVYxsVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d78080710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Setting network parameters from after epoch %d\" %(best_params_epoch))\n",
    "load_parameters(best_params)\n",
    "\n",
    "print(\"Test error rate is %f%%\" %(compute_error_rate(cifar_test_stream)*100.0,))\n",
    "\n",
    "subplot(2,1,1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:,0], train_nll_a[:,1], label='batch train nll')\n",
    "legend()\n",
    "\n",
    "subplot(2,1,2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:,0], train_erros_a[:,1], label='batch train error rate')\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:,0], validation_errors_a[:,1], label='validation error rate', color='r')\n",
    "ylim(0,0.2)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How do the filters in the first layer look like?\n",
    "\n",
    "plot_mat(CW1.get_value(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iii = predict.maker.inputs[0]\n",
    "X = iii.variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a function that shows how the network processes an image\n",
    "\n",
    "middle_layers_computer = theano.function([X], [\n",
    "        X,\n",
    "        after_C1,\n",
    "        after_P1,\n",
    "        after_C2,\n",
    "        after_P2\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_num=4\n",
    "\n",
    "middle_layers = middle_layers_computer(X_test_value[img_num:img_num+1])\n",
    "\n",
    "for ml, name in zip(middle_layers, ['X', 'C1', 'P1', 'C2', 'P2']):\n",
    "    plot_mat(ml.transpose(1,0,2,3), cmap='gray')\n",
    "    title(name)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
