{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "from scipy.ndimage.filters import convolve\n",
    "\n",
    "#note: this requires the starter code for the assignments!\n",
    "from common.plotting import plot_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 740M\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST example\n",
    "\n",
    "We will now build a convolutional network for the MNIST data. We will use Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 25))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 100))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing ('features', 'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 1, 28, 28) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 1, 28, 28) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print(\"The streams return batches containing %s\" % (mnist_train_stream.sources,))\n",
    "\n",
    "print(\"Each trainin batch consits of a tuple containing:\")\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))\n",
    "    \n",
    "print(\"Validation/test batches consits of tuples containing:\")\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X type: float32\n",
      "X shape: (3, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4('X', dtype='float32')\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix('Y', dtype='uint8')\n",
    "\n",
    "#The tag values are useful during debugging the creation of Theano graphs\n",
    "\n",
    "X_test_value, Y_test_value = next(mnist_train_stream.get_epoch_iterator())\n",
    "#\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "#\n",
    "theano.config.compute_test_value = 'off' # Enable the computation of test values\n",
    "\n",
    "\n",
    "X.tag.test_value = X_test_value[:3]\n",
    "Y.tag.test_value = Y_test_value[:3]\n",
    "\n",
    "print(\"X type: %s\" % X.tag.test_value.dtype)\n",
    "print(\"X shape: %s\" % (X.tag.test_value.shape,))\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "#The first convolutional layer\n",
    "#The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = 10 #we will apply that many convolution filters in the first layer\n",
    "CW1 = theano.shared(np.zeros((num_filters_1,1,5,5), dtype='float32'),\n",
    "                   name='CW1')\n",
    "#please note - this is somewhat non-standard\n",
    "CW1.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype='float32'),\n",
    "                    name='CB1')\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "# print \"after_C1 shape: %s\" % (after_C1.tag.test_value.shape,)\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2,2), ignore_border=True)\n",
    "# print \"after_P1 shape: %s\" % (after_P1.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_filters_2 = 25 #we will compute ten convolution filters in the first layer\n",
    "CW2 = theano.shared(np.zeros((num_filters_2,num_filters_1,5,5), dtype='float32'),\n",
    "                   name='CW2')\n",
    "CW2.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype='float32'),\n",
    "                    name='CB2')\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "# print \"after_C2 shape: %s\" % (after_C2.tag.test_value.shape,)\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2,2), ignore_border=True)\n",
    "# print \"after_P2 shape: %s\" % (after_P2.tag.test_value.shape,)\n",
    "\n",
    "#Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden=500\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 4 * 4, num_fw3_hidden), dtype='float32'),\n",
    "                   name='FW3')\n",
    "FW3.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype='float32'),\n",
    "                    name='FB3')\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle('x',0))\n",
    "# print \"after_F3 shape: %s\" % (after_F3.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_fw4_hidden=10\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype='float32'),\n",
    "                   name='FW4')\n",
    "FW4.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype='float32'),\n",
    "                    name='FB4')\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle('x',0)\n",
    "# print \"after_F4 shape: %s\" % (after_F4.tag.test_value.shape,)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis=1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1]=='W':\n",
    "        weight_decay = weight_decay + 1e-3 * (p**2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "#At this point stop computing test values\n",
    "theano.config.compute_test_value = 'off' # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"2236pt\" viewBox=\"0.00 0.00 2889.00 2236.00\" width=\"2889pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 2232)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-2232 2885,-2232 2885,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- Flatten{1} -->\n",
       "<g class=\"node\" id=\"node1\"><title>Flatten{1}</title>\n",
       "<ellipse cx=\"48\" cy=\"-884\" fill=\"none\" rx=\"48.1437\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"48\" y=\"-880.3\">Flatten{1}</text>\n",
       "</g>\n",
       "<!-- AdvancedSubtensor -->\n",
       "<g class=\"node\" id=\"node49\"><title>AdvancedSubtensor</title>\n",
       "<ellipse cx=\"400\" cy=\"-654\" fill=\"#ffaaff\" rx=\"81.3646\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"400\" y=\"-650.3\">AdvancedSubtensor</text>\n",
       "</g>\n",
       "<!-- Flatten{1}&#45;&gt;AdvancedSubtensor -->\n",
       "<g class=\"edge\" id=\"edge51\"><title>Flatten{1}-&gt;AdvancedSubtensor</title>\n",
       "<path d=\"M47.41,-865.896C47.5551,-843.093 51.1944,-802.743 73,-778 135.527,-707.051 243.188,-676.899 317.811,-664.167\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"318.808,-667.551 328.113,-662.484 317.68,-660.642 318.808,-667.551\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"150\" y=\"-782.3\">2 TensorType(uint8, vector)</text>\n",
       "</g>\n",
       "<!-- name=Y TensorType(uint8, matrix) -->\n",
       "<g class=\"node\" id=\"node2\"><title>name=Y TensorType(uint8, matrix)</title>\n",
       "<polygon fill=\"green\" points=\"239.5,-1044 26.5,-1044 26.5,-1008 239.5,-1008 239.5,-1044\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133\" y=\"-1022.3\">name=Y TensorType(uint8, matrix)</text>\n",
       "</g>\n",
       "<!-- name=Y TensorType(uint8, matrix)&#45;&gt;Flatten{1} -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>name=Y TensorType(uint8, matrix)-&gt;Flatten{1}</title>\n",
       "<path d=\"M64.83,-1007.99C57.0345,-1003.37 50.0771,-997.483 45,-990 29.4619,-967.098 33.4001,-934.45 39.1838,-911.659\" fill=\"none\" stroke=\"blue\"/>\n",
       "<polygon fill=\"blue\" points=\"42.5758,-912.526 41.9076,-901.952 35.8361,-910.634 42.5758,-912.526\" stroke=\"blue\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118\" y=\"-978.3\">TensorType(uint8, matrix)</text>\n",
       "</g>\n",
       "<!-- Shape -->\n",
       "<g class=\"node\" id=\"node3\"><title>Shape</title>\n",
       "<ellipse cx=\"216\" cy=\"-938\" fill=\"cyan\" rx=\"32.2457\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"216\" y=\"-934.3\">Shape</text>\n",
       "</g>\n",
       "<!-- name=Y TensorType(uint8, matrix)&#45;&gt;Shape -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>name=Y TensorType(uint8, matrix)-&gt;Shape</title>\n",
       "<path d=\"M172.625,-1007.86C180.788,-1002.98 188.779,-997.039 195,-990 201.107,-983.089 205.591,-974.16 208.809,-965.709\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"212.151,-966.752 212.027,-956.157 205.517,-964.517 212.151,-966.752\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278\" y=\"-978.3\">TensorType(uint8, matrix)</text>\n",
       "</g>\n",
       "<!-- Subtensor{int64} -->\n",
       "<g class=\"node\" id=\"node24\"><title>Subtensor{int64}</title>\n",
       "<ellipse cx=\"216\" cy=\"-830\" fill=\"#ffaaff\" rx=\"72.192\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"216\" y=\"-826.3\">Subtensor{int64}</text>\n",
       "</g>\n",
       "<!-- Shape&#45;&gt;Subtensor{int64} -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>Shape-&gt;Subtensor{int64}</title>\n",
       "<path d=\"M216,-919.969C216,-903.378 216,-877.883 216,-858.431\" fill=\"none\" stroke=\"blue\"/>\n",
       "<polygon fill=\"blue\" points=\"219.5,-858.341 216,-848.341 212.5,-858.341 219.5,-858.341\" stroke=\"blue\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293\" y=\"-880.3\">0 TensorType(int64, vector)</text>\n",
       "</g>\n",
       "<!-- HostFromGpu -->\n",
       "<g class=\"node\" id=\"node4\"><title>HostFromGpu</title>\n",
       "<ellipse cx=\"719\" cy=\"-1026\" fill=\"red\" rx=\"61.6163\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"719\" y=\"-1022.3\">HostFromGpu</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,0} -->\n",
       "<g class=\"node\" id=\"node26\"><title>DimShuffle{x,0}</title>\n",
       "<ellipse cx=\"746\" cy=\"-938\" fill=\"none\" rx=\"72.192\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"746\" y=\"-934.3\">DimShuffle{x,0}</text>\n",
       "</g>\n",
       "<!-- HostFromGpu&#45;&gt;DimShuffle{x,0} -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>HostFromGpu-&gt;DimShuffle{x,0}</title>\n",
       "<path d=\"M724.334,-1008.01C728.106,-995.995 733.241,-979.641 737.564,-965.869\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"741.021,-966.545 740.677,-955.956 734.342,-964.448 741.021,-966.545\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"812\" y=\"-978.3\">TensorType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- name=FB4 CudaNdarrayType(float32, vector) -->\n",
       "<g class=\"node\" id=\"node5\"><title>name=FB4 CudaNdarrayType(float32, vector)</title>\n",
       "<polygon fill=\"green\" points=\"808.25,-1132 537.75,-1132 537.75,-1096 808.25,-1096 808.25,-1132\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"673\" y=\"-1110.3\">name=FB4 CudaNdarrayType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- name=FB4 CudaNdarrayType(float32, vector)&#45;&gt;HostFromGpu -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>name=FB4 CudaNdarrayType(float32, vector)-&gt;HostFromGpu</title>\n",
       "<path d=\"M682.309,-1095.6C688.836,-1083.39 697.676,-1066.87 705.043,-1053.09\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"708.232,-1054.55 709.862,-1044.08 702.059,-1051.25 708.232,-1054.55\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"796\" y=\"-1066.3\">CudaNdarrayType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=3 -->\n",
       "<g class=\"node\" id=\"node6\"><title>HostFromGpu id=3</title>\n",
       "<ellipse cx=\"942\" cy=\"-1026\" fill=\"red\" rx=\"81.3646\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"942\" y=\"-1022.3\">HostFromGpu id=3</text>\n",
       "</g>\n",
       "<!-- dot id=31 -->\n",
       "<g class=\"node\" id=\"node46\"><title>dot id=31</title>\n",
       "<ellipse cx=\"942\" cy=\"-938\" fill=\"none\" rx=\"45.244\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"942\" y=\"-934.3\">dot id=31</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=3&#45;&gt;dot id=31 -->\n",
       "<g class=\"edge\" id=\"edge45\"><title>HostFromGpu id=3-&gt;dot id=31</title>\n",
       "<path d=\"M942,-1007.6C942,-995.746 942,-979.817 942,-966.292\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"945.5,-966.084 942,-956.084 938.5,-966.084 945.5,-966.084\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1025.5\" y=\"-978.3\">1 TensorType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- name=FW4 CudaNdarrayType(float32, matrix) -->\n",
       "<g class=\"node\" id=\"node7\"><title>name=FW4 CudaNdarrayType(float32, matrix)</title>\n",
       "<polygon fill=\"green\" points=\"1105,-1132 827,-1132 827,-1096 1105,-1096 1105,-1132\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"966\" y=\"-1110.3\">name=FW4 CudaNdarrayType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- name=FW4 CudaNdarrayType(float32, matrix)&#45;&gt;HostFromGpu id=3 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>name=FW4 CudaNdarrayType(float32, matrix)-&gt;HostFromGpu id=3</title>\n",
       "<path d=\"M961.143,-1095.6C957.803,-1083.63 953.303,-1067.5 949.504,-1053.89\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"952.827,-1052.78 946.768,-1044.08 946.085,-1054.66 952.827,-1052.78\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1053\" y=\"-1066.3\">CudaNdarrayType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=4 -->\n",
       "<g class=\"node\" id=\"node8\"><title>HostFromGpu id=4</title>\n",
       "<ellipse cx=\"1349\" cy=\"-1290\" fill=\"red\" rx=\"81.3646\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1349\" y=\"-1286.3\">HostFromGpu id=4</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,0} id=15 -->\n",
       "<g class=\"node\" id=\"node27\"><title>DimShuffle{x,0} id=15</title>\n",
       "<ellipse cx=\"1349\" cy=\"-1202\" fill=\"none\" rx=\"96.2874\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1349\" y=\"-1198.3\">DimShuffle{x,0} id=15</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=4&#45;&gt;DimShuffle{x,0} id=15 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>HostFromGpu id=4-&gt;DimShuffle{x,0} id=15</title>\n",
       "<path d=\"M1349,-1271.6C1349,-1259.75 1349,-1243.82 1349,-1230.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1352.5,-1230.08 1349,-1220.08 1345.5,-1230.08 1352.5,-1230.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1426\" y=\"-1242.3\">TensorType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- name=FB3 CudaNdarrayType(float32, vector) -->\n",
       "<g class=\"node\" id=\"node9\"><title>name=FB3 CudaNdarrayType(float32, vector)</title>\n",
       "<polygon fill=\"green\" points=\"1484.25,-1396 1213.75,-1396 1213.75,-1360 1484.25,-1360 1484.25,-1396\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1349\" y=\"-1374.3\">name=FB3 CudaNdarrayType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- name=FB3 CudaNdarrayType(float32, vector)&#45;&gt;HostFromGpu id=4 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>name=FB3 CudaNdarrayType(float32, vector)-&gt;HostFromGpu id=4</title>\n",
       "<path d=\"M1349,-1359.6C1349,-1347.75 1349,-1331.82 1349,-1318.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1352.5,-1318.08 1349,-1308.08 1345.5,-1318.08 1352.5,-1318.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1444\" y=\"-1330.3\">CudaNdarrayType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=5 -->\n",
       "<g class=\"node\" id=\"node10\"><title>HostFromGpu id=5</title>\n",
       "<ellipse cx=\"1642\" cy=\"-1290\" fill=\"red\" rx=\"81.3646\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1642\" y=\"-1286.3\">HostFromGpu id=5</text>\n",
       "</g>\n",
       "<!-- dot -->\n",
       "<g class=\"node\" id=\"node43\"><title>dot</title>\n",
       "<ellipse cx=\"1642\" cy=\"-1202\" fill=\"none\" rx=\"27\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1642\" y=\"-1198.3\">dot</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=5&#45;&gt;dot -->\n",
       "<g class=\"edge\" id=\"edge39\"><title>HostFromGpu id=5-&gt;dot</title>\n",
       "<path d=\"M1642,-1271.6C1642,-1259.75 1642,-1243.82 1642,-1230.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1645.5,-1230.08 1642,-1220.08 1638.5,-1230.08 1645.5,-1230.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1725.5\" y=\"-1242.3\">1 TensorType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- name=FW3 CudaNdarrayType(float32, matrix) -->\n",
       "<g class=\"node\" id=\"node11\"><title>name=FW3 CudaNdarrayType(float32, matrix)</title>\n",
       "<polygon fill=\"green\" points=\"1781,-1396 1503,-1396 1503,-1360 1781,-1360 1781,-1396\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1642\" y=\"-1374.3\">name=FW3 CudaNdarrayType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- name=FW3 CudaNdarrayType(float32, matrix)&#45;&gt;HostFromGpu id=5 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>name=FW3 CudaNdarrayType(float32, matrix)-&gt;HostFromGpu id=5</title>\n",
       "<path d=\"M1642,-1359.6C1642,-1347.75 1642,-1331.82 1642,-1318.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1645.5,-1318.08 1642,-1308.08 1638.5,-1318.08 1645.5,-1318.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1738\" y=\"-1330.3\">CudaNdarrayType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=6 -->\n",
       "<g class=\"node\" id=\"node12\"><title>HostFromGpu id=6</title>\n",
       "<ellipse cx=\"1876\" cy=\"-1730\" fill=\"red\" rx=\"81.3646\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1876\" y=\"-1726.3\">HostFromGpu id=6</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,0,x,x} -->\n",
       "<g class=\"node\" id=\"node28\"><title>DimShuffle{x,0,x,x}</title>\n",
       "<ellipse cx=\"1980\" cy=\"-1642\" fill=\"none\" rx=\"86.1637\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1980\" y=\"-1638.3\">DimShuffle{x,0,x,x}</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=6&#45;&gt;DimShuffle{x,0,x,x} -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>HostFromGpu id=6-&gt;DimShuffle{x,0,x,x}</title>\n",
       "<path d=\"M1896.05,-1712.42C1912.02,-1699.21 1934.56,-1680.58 1952.26,-1665.94\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1954.53,-1668.6 1960.01,-1659.53 1950.07,-1663.21 1954.53,-1668.6\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2015\" y=\"-1682.3\">TensorType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- name=CB2 CudaNdarrayType(float32, vector) -->\n",
       "<g class=\"node\" id=\"node13\"><title>name=CB2 CudaNdarrayType(float32, vector)</title>\n",
       "<polygon fill=\"green\" points=\"1961,-1856 1689,-1856 1689,-1820 1961,-1820 1961,-1856\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1825\" y=\"-1834.3\">name=CB2 CudaNdarrayType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- name=CB2 CudaNdarrayType(float32, vector)&#45;&gt;HostFromGpu id=6 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>name=CB2 CudaNdarrayType(float32, vector)-&gt;HostFromGpu id=6</title>\n",
       "<path d=\"M1833.19,-1819.97C1841.37,-1802.97 1854.05,-1776.62 1863.49,-1757.01\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1866.64,-1758.52 1867.83,-1747.99 1860.34,-1755.48 1866.64,-1758.52\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1954\" y=\"-1780.3\">CudaNdarrayType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=7 -->\n",
       "<g class=\"node\" id=\"node14\"><title>HostFromGpu id=7</title>\n",
       "<ellipse cx=\"2106\" cy=\"-1730\" fill=\"red\" rx=\"81.3646\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2106\" y=\"-1726.3\">HostFromGpu id=7</text>\n",
       "</g>\n",
       "<!-- ConvOp{(&#39;imshp&#39;, (None, None, None)),(&#39;kshp&#39;, (None, None)),(&#39;... id=2 -->\n",
       "<g class=\"node\" id=\"node38\"><title>ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('... id=2</title>\n",
       "<ellipse cx=\"2352\" cy=\"-1642\" fill=\"none\" rx=\"267.165\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2352\" y=\"-1638.3\">ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('... id=2</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=7&#45;&gt;ConvOp{(&#39;imshp&#39;, (None, None, None)),(&#39;kshp&#39;, (None, None)),(&#39;... id=2 -->\n",
       "<g class=\"edge\" id=\"edge31\"><title>HostFromGpu id=7-&gt;ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('... id=2</title>\n",
       "<path d=\"M2137.08,-1713.28C2159.47,-1702.38 2190.6,-1688.07 2219,-1678 2235.55,-1672.13 2253.58,-1666.75 2270.86,-1662.05\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2272.17,-1665.32 2280.93,-1659.37 2270.37,-1658.56 2272.17,-1665.32\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2293\" y=\"-1682.3\">1 TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- name=CW2 CudaNdarrayType(float32, 4D) -->\n",
       "<g class=\"node\" id=\"node15\"><title>name=CW2 CudaNdarrayType(float32, 4D)</title>\n",
       "<polygon fill=\"green\" points=\"2239,-1856 1979,-1856 1979,-1820 2239,-1820 2239,-1856\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2109\" y=\"-1834.3\">name=CW2 CudaNdarrayType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- name=CW2 CudaNdarrayType(float32, 4D)&#45;&gt;HostFromGpu id=7 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>name=CW2 CudaNdarrayType(float32, 4D)-&gt;HostFromGpu id=7</title>\n",
       "<path d=\"M2108.52,-1819.97C2108.05,-1803.38 2107.33,-1777.88 2106.78,-1758.43\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2110.27,-1758.24 2106.49,-1748.34 2103.28,-1758.44 2110.27,-1758.24\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2194\" y=\"-1780.3\">CudaNdarrayType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=8 -->\n",
       "<g class=\"node\" id=\"node16\"><title>HostFromGpu id=8</title>\n",
       "<ellipse cx=\"2210\" cy=\"-2122\" fill=\"red\" rx=\"81.3646\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2210\" y=\"-2118.3\">HostFromGpu id=8</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,0,x,x} id=17 -->\n",
       "<g class=\"node\" id=\"node29\"><title>DimShuffle{x,0,x,x} id=17</title>\n",
       "<ellipse cx=\"2210\" cy=\"-2034\" fill=\"none\" rx=\"109.261\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2210\" y=\"-2030.3\">DimShuffle{x,0,x,x} id=17</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=8&#45;&gt;DimShuffle{x,0,x,x} id=17 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>HostFromGpu id=8-&gt;DimShuffle{x,0,x,x} id=17</title>\n",
       "<path d=\"M2210,-2103.6C2210,-2091.75 2210,-2075.82 2210,-2062.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2213.5,-2062.08 2210,-2052.08 2206.5,-2062.08 2213.5,-2062.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2287\" y=\"-2074.3\">TensorType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- name=CB1 CudaNdarrayType(float32, vector) -->\n",
       "<g class=\"node\" id=\"node17\"><title>name=CB1 CudaNdarrayType(float32, vector)</title>\n",
       "<polygon fill=\"green\" points=\"2346,-2228 2074,-2228 2074,-2192 2346,-2192 2346,-2228\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2210\" y=\"-2206.3\">name=CB1 CudaNdarrayType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- name=CB1 CudaNdarrayType(float32, vector)&#45;&gt;HostFromGpu id=8 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>name=CB1 CudaNdarrayType(float32, vector)-&gt;HostFromGpu id=8</title>\n",
       "<path d=\"M2210,-2191.6C2210,-2179.75 2210,-2163.82 2210,-2150.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2213.5,-2150.08 2210,-2140.08 2206.5,-2150.08 2213.5,-2150.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2305\" y=\"-2162.3\">CudaNdarrayType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=9 -->\n",
       "<g class=\"node\" id=\"node18\"><title>HostFromGpu id=9</title>\n",
       "<ellipse cx=\"2542\" cy=\"-2122\" fill=\"red\" rx=\"81.3646\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2542\" y=\"-2118.3\">HostFromGpu id=9</text>\n",
       "</g>\n",
       "<!-- ConvOp{(&#39;imshp&#39;, (None, None, None)),(&#39;kshp&#39;, (None, None)),(&#39;nkern... -->\n",
       "<g class=\"node\" id=\"node30\"><title>ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern...</title>\n",
       "<ellipse cx=\"2606\" cy=\"-2034\" fill=\"none\" rx=\"268.139\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2606\" y=\"-2030.3\">ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern...</text>\n",
       "</g>\n",
       "<!-- HostFromGpu id=9&#45;&gt;ConvOp{(&#39;imshp&#39;, (None, None, None)),(&#39;kshp&#39;, (None, None)),(&#39;nkern... -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>HostFromGpu id=9-&gt;ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern...</title>\n",
       "<path d=\"M2551.64,-2103.96C2557.65,-2093.8 2565.77,-2080.81 2574,-2070 2576.58,-2066.61 2579.42,-2063.16 2582.32,-2059.81\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2585.01,-2062.05 2589.08,-2052.27 2579.8,-2057.38 2585.01,-2062.05\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2648\" y=\"-2074.3\">1 TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- name=CW1 CudaNdarrayType(float32, 4D) -->\n",
       "<g class=\"node\" id=\"node19\"><title>name=CW1 CudaNdarrayType(float32, 4D)</title>\n",
       "<polygon fill=\"green\" points=\"2672,-2228 2412,-2228 2412,-2192 2672,-2192 2672,-2228\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2542\" y=\"-2206.3\">name=CW1 CudaNdarrayType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- name=CW1 CudaNdarrayType(float32, 4D)&#45;&gt;HostFromGpu id=9 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>name=CW1 CudaNdarrayType(float32, 4D)-&gt;HostFromGpu id=9</title>\n",
       "<path d=\"M2542,-2191.6C2542,-2179.75 2542,-2163.82 2542,-2150.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2545.5,-2150.08 2542,-2140.08 2538.5,-2150.08 2545.5,-2150.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2628\" y=\"-2162.3\">CudaNdarrayType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,x,x,x} -->\n",
       "<g class=\"node\" id=\"node20\"><title>DimShuffle{x,x,x,x}</title>\n",
       "<ellipse cx=\"1988\" cy=\"-1946\" fill=\"none\" rx=\"86.1637\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1988\" y=\"-1942.3\">DimShuffle{x,x,x,x}</text>\n",
       "</g>\n",
       "<!-- Elemwise{maximum,no_inplace} -->\n",
       "<g class=\"node\" id=\"node36\"><title>Elemwise{maximum,no_inplace}</title>\n",
       "<ellipse cx=\"2396\" cy=\"-1838\" fill=\"#ffaabb\" rx=\"132.382\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2396\" y=\"-1834.3\">Elemwise{maximum,no_inplace}</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,x,x,x}&#45;&gt;Elemwise{maximum,no_inplace} -->\n",
       "<g class=\"edge\" id=\"edge27\"><title>DimShuffle{x,x,x,x}-&gt;Elemwise{maximum,no_inplace}</title>\n",
       "<path d=\"M2013.5,-1928.57C2040.55,-1911.98 2085.07,-1887.01 2127,-1874 2142.54,-1869.18 2219.65,-1859.37 2287.35,-1851.36\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2288.17,-1854.79 2297.7,-1850.14 2287.35,-1847.83 2288.17,-1854.79\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2259.5\" y=\"-1888.3\">0 TensorType(float32, (True, True, True, True))</text>\n",
       "</g>\n",
       "<!-- val=0.0 TensorType(float32, scalar) -->\n",
       "<g class=\"node\" id=\"node21\"><title>val=0.0 TensorType(float32, scalar)</title>\n",
       "<polygon fill=\"green\" points=\"1717.5,-2052 1504.5,-2052 1504.5,-2016 1717.5,-2016 1717.5,-2052\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1611\" y=\"-2030.3\">val=0.0 TensorType(float32, scalar)</text>\n",
       "</g>\n",
       "<!-- val=0.0 TensorType(float32, scalar)&#45;&gt;DimShuffle{x,x,x,x} -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>val=0.0 TensorType(float32, scalar)-&gt;DimShuffle{x,x,x,x}</title>\n",
       "<path d=\"M1685.93,-2015.91C1754.97,-2000.16 1856.41,-1977.02 1922.34,-1961.98\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1923.23,-1965.37 1932.2,-1959.73 1921.67,-1958.54 1923.23,-1965.37\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1910\" y=\"-1986.3\">TensorType(float32, scalar)</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,x,x,x} id=11 -->\n",
       "<g class=\"node\" id=\"node22\"><title>DimShuffle{x,x,x,x} id=11</title>\n",
       "<ellipse cx=\"1611\" cy=\"-1784\" fill=\"none\" rx=\"109.261\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1611\" y=\"-1780.3\">DimShuffle{x,x,x,x} id=11</text>\n",
       "</g>\n",
       "<!-- val=0.0 TensorType(float32, scalar)&#45;&gt;DimShuffle{x,x,x,x} id=11 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>val=0.0 TensorType(float32, scalar)-&gt;DimShuffle{x,x,x,x} id=11</title>\n",
       "<path d=\"M1611,-2015.95C1611,-1973.66 1611,-1863.61 1611,-1812.2\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1614.5,-1812.17 1611,-1802.17 1607.5,-1812.17 1614.5,-1812.17\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1687\" y=\"-1942.3\">TensorType(float32, scalar)</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,x} -->\n",
       "<g class=\"node\" id=\"node23\"><title>DimShuffle{x,x}</title>\n",
       "<ellipse cx=\"1222\" cy=\"-1892\" fill=\"none\" rx=\"72.192\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1222\" y=\"-1888.3\">DimShuffle{x,x}</text>\n",
       "</g>\n",
       "<!-- val=0.0 TensorType(float32, scalar)&#45;&gt;DimShuffle{x,x} -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>val=0.0 TensorType(float32, scalar)-&gt;DimShuffle{x,x}</title>\n",
       "<path d=\"M1511.68,-2015.99C1489.47,-2011.1 1466.22,-2005.12 1445,-1998 1376.97,-1975.16 1302.38,-1937.19 1259.01,-1913.71\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1260.44,-1910.5 1249.99,-1908.79 1257.09,-1916.65 1260.44,-1910.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1521\" y=\"-1986.3\">TensorType(float32, scalar)</text>\n",
       "</g>\n",
       "<!-- Elemwise{maximum,no_inplace} id=25 -->\n",
       "<g class=\"node\" id=\"node40\"><title>Elemwise{maximum,no_inplace} id=25</title>\n",
       "<ellipse cx=\"2007\" cy=\"-1466\" fill=\"#ffaabb\" rx=\"155.479\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2007\" y=\"-1462.3\">Elemwise{maximum,no_inplace} id=25</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,x,x,x} id=11&#45;&gt;Elemwise{maximum,no_inplace} id=25 -->\n",
       "<g class=\"edge\" id=\"edge34\"><title>DimShuffle{x,x,x,x} id=11-&gt;Elemwise{maximum,no_inplace} id=25</title>\n",
       "<path d=\"M1611,-1765.66C1611,-1746.43 1611,-1714.5 1611,-1687 1611,-1687 1611,-1687 1611,-1553 1611,-1501.79 1739.06,-1481.01 1849.54,-1472.62\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1850.11,-1476.08 1859.83,-1471.86 1849.6,-1469.1 1850.11,-1476.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1743.5\" y=\"-1638.3\">0 TensorType(float32, (True, True, True, True))</text>\n",
       "</g>\n",
       "<!-- Elemwise{maximum,no_inplace} id=30 -->\n",
       "<g class=\"node\" id=\"node45\"><title>Elemwise{maximum,no_inplace} id=30</title>\n",
       "<ellipse cx=\"1197\" cy=\"-1026\" fill=\"#ffaabb\" rx=\"155.479\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1197\" y=\"-1022.3\">Elemwise{maximum,no_inplace} id=30</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,x}&#45;&gt;Elemwise{maximum,no_inplace} id=30 -->\n",
       "<g class=\"edge\" id=\"edge42\"><title>DimShuffle{x,x}-&gt;Elemwise{maximum,no_inplace} id=30</title>\n",
       "<path d=\"M1212.08,-1874.01C1201.12,-1853.58 1185,-1817.94 1185,-1785 1185,-1785 1185,-1785 1185,-1113 1185,-1093.21 1188.15,-1071.11 1191.23,-1054.27\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1194.74,-1054.52 1193.2,-1044.04 1187.87,-1053.19 1194.74,-1054.52\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1284.5\" y=\"-1462.3\">0 TensorType(float32, (True, True))</text>\n",
       "</g>\n",
       "<!-- ARange -->\n",
       "<g class=\"node\" id=\"node32\"><title>ARange</title>\n",
       "<ellipse cx=\"400\" cy=\"-742\" fill=\"none\" rx=\"39.4691\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"400\" y=\"-738.3\">ARange</text>\n",
       "</g>\n",
       "<!-- Subtensor{int64}&#45;&gt;ARange -->\n",
       "<g class=\"edge\" id=\"edge23\"><title>Subtensor{int64}-&gt;ARange</title>\n",
       "<path d=\"M221.261,-811.822C225.593,-800.542 232.859,-786.377 244,-778 274.529,-755.046 317.311,-746.656 350.354,-743.805\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"350.987,-747.269 360.711,-743.063 350.487,-740.287 350.987,-747.269\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"320\" y=\"-782.3\">1 TensorType(int64, scalar)</text>\n",
       "</g>\n",
       "<!-- val=0 int64 -->\n",
       "<g class=\"node\" id=\"node25\"><title>val=0 int64</title>\n",
       "<polygon fill=\"green\" points=\"433,-956 353,-956 353,-920 433,-920 433,-956\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"393\" y=\"-934.3\">val=0 int64</text>\n",
       "</g>\n",
       "<!-- val=0 int64&#45;&gt;Subtensor{int64} -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>val=0 int64-&gt;Subtensor{int64}</title>\n",
       "<path d=\"M392.378,-919.837C390.843,-903.643 385.971,-879.841 371,-866 369.195,-864.331 321.327,-853.666 279.261,-844.542\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"279.797,-841.077 269.283,-842.383 278.316,-847.919 279.797,-841.077\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"409.5\" y=\"-880.3\">1 int64</text>\n",
       "</g>\n",
       "<!-- Elemwise{add,no_inplace} id=32 -->\n",
       "<g class=\"node\" id=\"node47\"><title>Elemwise{add,no_inplace} id=32</title>\n",
       "<ellipse cx=\"847\" cy=\"-830\" fill=\"#ffaabb\" rx=\"131.408\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"847\" y=\"-826.3\">Elemwise{add,no_inplace} id=32</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,0}&#45;&gt;Elemwise{add,no_inplace} id=32 -->\n",
       "<g class=\"edge\" id=\"edge47\"><title>DimShuffle{x,0}-&gt;Elemwise{add,no_inplace} id=32</title>\n",
       "<path d=\"M744.437,-919.894C743.943,-904.195 745.71,-881.142 758,-866 762.929,-859.927 769.058,-854.904 775.778,-850.75\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"777.471,-853.813 784.56,-845.939 774.109,-847.673 777.471,-853.813\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"834\" y=\"-880.3\">1 TensorType(float32, row)</text>\n",
       "</g>\n",
       "<!-- Elemwise{add,no_inplace} id=29 -->\n",
       "<g class=\"node\" id=\"node44\"><title>Elemwise{add,no_inplace} id=29</title>\n",
       "<ellipse cx=\"1349\" cy=\"-1114\" fill=\"#ffaabb\" rx=\"131.408\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1349\" y=\"-1110.3\">Elemwise{add,no_inplace} id=29</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,0} id=15&#45;&gt;Elemwise{add,no_inplace} id=29 -->\n",
       "<g class=\"edge\" id=\"edge41\"><title>DimShuffle{x,0} id=15-&gt;Elemwise{add,no_inplace} id=29</title>\n",
       "<path d=\"M1349,-1183.6C1349,-1171.75 1349,-1155.82 1349,-1142.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1352.5,-1142.08 1349,-1132.08 1345.5,-1142.08 1352.5,-1142.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1425\" y=\"-1154.3\">1 TensorType(float32, row)</text>\n",
       "</g>\n",
       "<!-- Elemwise{add,no_inplace} id=24 -->\n",
       "<g class=\"node\" id=\"node39\"><title>Elemwise{add,no_inplace} id=24</title>\n",
       "<ellipse cx=\"2007\" cy=\"-1554\" fill=\"#ffaabb\" rx=\"131.408\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2007\" y=\"-1550.3\">Elemwise{add,no_inplace} id=24</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,0,x,x}&#45;&gt;Elemwise{add,no_inplace} id=24 -->\n",
       "<g class=\"edge\" id=\"edge33\"><title>DimShuffle{x,0,x,x}-&gt;Elemwise{add,no_inplace} id=24</title>\n",
       "<path d=\"M1985.33,-1624.01C1989.08,-1612.07 1994.17,-1595.85 1998.48,-1582.14\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2001.93,-1582.84 2001.58,-1572.25 1995.25,-1580.74 2001.93,-1582.84\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2130\" y=\"-1594.3\">1 TensorType(float32, (True, False, True, True))</text>\n",
       "</g>\n",
       "<!-- Elemwise{add,no_inplace} -->\n",
       "<g class=\"node\" id=\"node35\"><title>Elemwise{add,no_inplace}</title>\n",
       "<ellipse cx=\"2396\" cy=\"-1946\" fill=\"#ffaabb\" rx=\"108.31\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2396\" y=\"-1942.3\">Elemwise{add,no_inplace}</text>\n",
       "</g>\n",
       "<!-- DimShuffle{x,0,x,x} id=17&#45;&gt;Elemwise{add,no_inplace} -->\n",
       "<g class=\"edge\" id=\"edge26\"><title>DimShuffle{x,0,x,x} id=17-&gt;Elemwise{add,no_inplace}</title>\n",
       "<path d=\"M2223.93,-2016C2233.97,-2004.8 2248.45,-1990.66 2264,-1982 2279.15,-1973.56 2296.42,-1967.09 2313.36,-1962.15\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2314.57,-1965.45 2323.28,-1959.41 2312.71,-1958.7 2314.57,-1965.45\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2398\" y=\"-1986.3\">1 TensorType(float32, (True, False, True, True))</text>\n",
       "</g>\n",
       "<!-- ConvOp{(&#39;imshp&#39;, (None, None, None)),(&#39;kshp&#39;, (None, None)),(&#39;nkern...&#45;&gt;Elemwise{add,no_inplace} -->\n",
       "<g class=\"edge\" id=\"edge25\"><title>ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern...-&gt;Elemwise{add,no_inplace}</title>\n",
       "<path d=\"M2585.5,-2015.93C2571.28,-2004.85 2551.45,-1990.87 2532,-1982 2514.24,-1973.91 2494.29,-1967.45 2475.26,-1962.4\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2475.96,-1958.97 2465.4,-1959.89 2474.23,-1965.75 2475.96,-1958.97\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2633\" y=\"-1986.3\">0 TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- name=X TensorType(float32, 4D) -->\n",
       "<g class=\"node\" id=\"node31\"><title>name=X TensorType(float32, 4D)</title>\n",
       "<polygon fill=\"green\" points=\"2846.25,-2140 2641.75,-2140 2641.75,-2104 2846.25,-2104 2846.25,-2140\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2744\" y=\"-2118.3\">name=X TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- name=X TensorType(float32, 4D)&#45;&gt;ConvOp{(&#39;imshp&#39;, (None, None, None)),(&#39;kshp&#39;, (None, None)),(&#39;nkern... -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>name=X TensorType(float32, 4D)-&gt;ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern...</title>\n",
       "<path d=\"M2740.48,-2103.64C2737.38,-2092.58 2731.79,-2078.73 2722,-2070 2715.5,-2064.2 2708.06,-2059.36 2700.17,-2055.33\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2701.63,-2052.14 2691.09,-2051.1 2698.68,-2058.49 2701.63,-2052.14\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2807\" y=\"-2074.3\">0 TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- ARange&#45;&gt;AdvancedSubtensor -->\n",
       "<g class=\"edge\" id=\"edge50\"><title>ARange-&gt;AdvancedSubtensor</title>\n",
       "<path d=\"M400,-723.597C400,-711.746 400,-695.817 400,-682.292\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"403.5,-682.084 400,-672.084 396.5,-682.084 403.5,-682.084\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"477\" y=\"-694.3\">1 TensorType(int64, vector)</text>\n",
       "</g>\n",
       "<!-- val=0 TensorType(int8, scalar) -->\n",
       "<g class=\"node\" id=\"node33\"><title>val=0 TensorType(int8, scalar)</title>\n",
       "<polygon fill=\"green\" points=\"492.5,-848 307.5,-848 307.5,-812 492.5,-812 492.5,-848\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"400\" y=\"-826.3\">val=0 TensorType(int8, scalar)</text>\n",
       "</g>\n",
       "<!-- val=0 TensorType(int8, scalar)&#45;&gt;ARange -->\n",
       "<g class=\"edge\" id=\"edge22\"><title>val=0 TensorType(int8, scalar)-&gt;ARange</title>\n",
       "<path d=\"M400,-811.597C400,-799.746 400,-783.817 400,-770.292\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"403.5,-770.084 400,-760.084 396.5,-770.084 403.5,-770.084\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"472.5\" y=\"-782.3\">0 TensorType(int8, scalar)</text>\n",
       "</g>\n",
       "<!-- val=1 TensorType(int8, scalar) -->\n",
       "<g class=\"node\" id=\"node34\"><title>val=1 TensorType(int8, scalar)</title>\n",
       "<polygon fill=\"green\" points=\"696.5,-848 511.5,-848 511.5,-812 696.5,-812 696.5,-848\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"604\" y=\"-826.3\">val=1 TensorType(int8, scalar)</text>\n",
       "</g>\n",
       "<!-- val=1 TensorType(int8, scalar)&#45;&gt;ARange -->\n",
       "<g class=\"edge\" id=\"edge24\"><title>val=1 TensorType(int8, scalar)-&gt;ARange</title>\n",
       "<path d=\"M589.827,-811.865C579.606,-800.605 564.863,-786.445 549,-778 517.706,-761.34 478.561,-752.539 448.247,-747.93\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"448.722,-744.463 438.33,-746.523 447.739,-751.393 448.722,-744.463\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"643.5\" y=\"-782.3\">2 TensorType(int8, scalar)</text>\n",
       "</g>\n",
       "<!-- Elemwise{add,no_inplace}&#45;&gt;Elemwise{maximum,no_inplace} -->\n",
       "<g class=\"edge\" id=\"edge28\"><title>Elemwise{add,no_inplace}-&gt;Elemwise{maximum,no_inplace}</title>\n",
       "<path d=\"M2396,-1927.97C2396,-1911.38 2396,-1885.88 2396,-1866.43\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2399.5,-1866.34 2396,-1856.34 2392.5,-1866.34 2399.5,-1866.34\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2470\" y=\"-1888.3\">1 TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)} -->\n",
       "<g class=\"node\" id=\"node37\"><title>DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)}</title>\n",
       "<ellipse cx=\"2400\" cy=\"-1730\" fill=\"none\" rx=\"194.474\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2400\" y=\"-1726.3\">DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)}</text>\n",
       "</g>\n",
       "<!-- Elemwise{maximum,no_inplace}&#45;&gt;DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)} -->\n",
       "<g class=\"edge\" id=\"edge29\"><title>Elemwise{maximum,no_inplace}-&gt;DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)}</title>\n",
       "<path d=\"M2396.64,-1819.97C2397.27,-1803.38 2398.23,-1777.88 2398.96,-1758.43\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2402.47,-1758.47 2399.35,-1748.34 2395.47,-1758.2 2402.47,-1758.47\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2466.5\" y=\"-1780.3\">TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)}&#45;&gt;ConvOp{(&#39;imshp&#39;, (None, None, None)),(&#39;kshp&#39;, (None, None)),(&#39;... id=2 -->\n",
       "<g class=\"edge\" id=\"edge30\"><title>DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)}-&gt;ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('... id=2</title>\n",
       "<path d=\"M2390.52,-1712.01C2383.72,-1699.84 2374.45,-1683.22 2366.7,-1669.33\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2369.56,-1667.28 2361.63,-1660.25 2363.45,-1670.69 2369.56,-1667.28\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2454\" y=\"-1682.3\">0 TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- ConvOp{(&#39;imshp&#39;, (None, None, None)),(&#39;kshp&#39;, (None, None)),(&#39;... id=2&#45;&gt;Elemwise{add,no_inplace} id=24 -->\n",
       "<g class=\"edge\" id=\"edge32\"><title>ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('... id=2-&gt;Elemwise{add,no_inplace} id=24</title>\n",
       "<path d=\"M2329.76,-1623.97C2313.53,-1612.46 2290.52,-1597.95 2268,-1590 2227.19,-1575.59 2180.88,-1567.05 2138.71,-1562.01\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2138.98,-1558.52 2128.65,-1560.86 2138.19,-1565.47 2138.98,-1558.52\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2373\" y=\"-1594.3\">0 TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- Elemwise{add,no_inplace} id=24&#45;&gt;Elemwise{maximum,no_inplace} id=25 -->\n",
       "<g class=\"edge\" id=\"edge35\"><title>Elemwise{add,no_inplace} id=24-&gt;Elemwise{maximum,no_inplace} id=25</title>\n",
       "<path d=\"M2007,-1535.6C2007,-1523.75 2007,-1507.82 2007,-1494.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2010.5,-1494.08 2007,-1484.08 2003.5,-1494.08 2010.5,-1494.08\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2081\" y=\"-1506.3\">1 TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)} id=26 -->\n",
       "<g class=\"node\" id=\"node41\"><title>DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)} id=26</title>\n",
       "<ellipse cx=\"2017\" cy=\"-1378\" fill=\"none\" rx=\"218.071\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2017\" y=\"-1374.3\">DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)} id=26</text>\n",
       "</g>\n",
       "<!-- Elemwise{maximum,no_inplace} id=25&#45;&gt;DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)} id=26 -->\n",
       "<g class=\"edge\" id=\"edge36\"><title>Elemwise{maximum,no_inplace} id=25-&gt;DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)} id=26</title>\n",
       "<path d=\"M2009.02,-1447.6C2010.4,-1435.75 2012.25,-1419.82 2013.83,-1406.29\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"2017.33,-1406.42 2015.01,-1396.08 2010.38,-1405.61 2017.33,-1406.42\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2081.5\" y=\"-1418.3\">TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- Flatten{2} -->\n",
       "<g class=\"node\" id=\"node42\"><title>Flatten{2}</title>\n",
       "<ellipse cx=\"1910\" cy=\"-1290\" fill=\"none\" rx=\"48.1437\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1910\" y=\"-1286.3\">Flatten{2}</text>\n",
       "</g>\n",
       "<!-- DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)} id=26&#45;&gt;Flatten{2} -->\n",
       "<g class=\"edge\" id=\"edge37\"><title>DownsampleFactorMax{(2, 2), (2, 2), True, (0, 0)} id=26-&gt;Flatten{2}</title>\n",
       "<path d=\"M1995.86,-1360.01C1979.06,-1346.51 1955.45,-1327.53 1937.24,-1312.9\" fill=\"none\" stroke=\"blue\"/>\n",
       "<polygon fill=\"blue\" points=\"1939.28,-1310.05 1929.3,-1306.51 1934.9,-1315.5 1939.28,-1310.05\" stroke=\"blue\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2042.5\" y=\"-1330.3\">TensorType(float32, 4D)</text>\n",
       "</g>\n",
       "<!-- Flatten{2}&#45;&gt;dot -->\n",
       "<g class=\"edge\" id=\"edge38\"><title>Flatten{2}-&gt;dot</title>\n",
       "<path d=\"M1886.54,-1274.25C1867.41,-1262.83 1839.34,-1247.41 1813,-1238 1768.13,-1221.98 1713.87,-1212.43 1678.68,-1207.45\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1678.79,-1203.94 1668.4,-1206.05 1677.84,-1210.87 1678.79,-1203.94\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1932.5\" y=\"-1242.3\">0 TensorType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- dot&#45;&gt;Elemwise{add,no_inplace} id=29 -->\n",
       "<g class=\"edge\" id=\"edge40\"><title>dot-&gt;Elemwise{add,no_inplace} id=29</title>\n",
       "<path d=\"M1619.53,-1191.56C1592.65,-1180.52 1546.12,-1162.2 1505,-1150 1481.26,-1142.95 1455.12,-1136.57 1431.14,-1131.24\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1431.88,-1127.82 1421.36,-1129.1 1430.38,-1134.66 1431.88,-1127.82\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1634.5\" y=\"-1154.3\">0 TensorType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- Elemwise{add,no_inplace} id=29&#45;&gt;Elemwise{maximum,no_inplace} id=30 -->\n",
       "<g class=\"edge\" id=\"edge43\"><title>Elemwise{add,no_inplace} id=29-&gt;Elemwise{maximum,no_inplace} id=30</title>\n",
       "<path d=\"M1319.69,-1096.42C1295.69,-1082.84 1261.55,-1063.52 1235.37,-1048.71\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1236.9,-1045.56 1226.48,-1043.68 1233.46,-1051.65 1236.9,-1045.56\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1370.5\" y=\"-1066.3\">1 TensorType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- Elemwise{maximum,no_inplace} id=30&#45;&gt;dot id=31 -->\n",
       "<g class=\"edge\" id=\"edge44\"><title>Elemwise{maximum,no_inplace} id=30-&gt;dot id=31</title>\n",
       "<path d=\"M1173.88,-1007.96C1157.62,-996.745 1134.92,-982.597 1113,-974 1075.03,-959.11 1029.73,-950.202 995.292,-945.103\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"995.366,-941.578 984.974,-943.64 994.384,-948.509 995.366,-941.578\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1228.5\" y=\"-978.3\">0 TensorType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- dot id=31&#45;&gt;Elemwise{add,no_inplace} id=32 -->\n",
       "<g class=\"edge\" id=\"edge46\"><title>dot id=31-&gt;Elemwise{add,no_inplace} id=32</title>\n",
       "<path d=\"M938.848,-919.998C935.197,-904.371 927.823,-881.368 914,-866 909.401,-860.887 903.877,-856.4 897.999,-852.498\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"899.573,-849.36 889.202,-847.194 895.959,-855.355 899.573,-849.36\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1017.5\" y=\"-880.3\">0 TensorType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- Softmax -->\n",
       "<g class=\"node\" id=\"node48\"><title>Softmax</title>\n",
       "<ellipse cx=\"751\" cy=\"-742\" fill=\"none\" rx=\"40.4202\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"751\" y=\"-738.3\">Softmax</text>\n",
       "</g>\n",
       "<!-- Elemwise{add,no_inplace} id=32&#45;&gt;Softmax -->\n",
       "<g class=\"edge\" id=\"edge48\"><title>Elemwise{add,no_inplace} id=32-&gt;Softmax</title>\n",
       "<path d=\"M828.033,-812.009C813.101,-798.632 792.162,-779.875 775.884,-765.292\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"778.097,-762.576 768.314,-758.51 773.427,-767.79 778.097,-762.576\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"886.5\" y=\"-782.3\">TensorType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- Softmax&#45;&gt;AdvancedSubtensor -->\n",
       "<g class=\"edge\" id=\"edge49\"><title>Softmax-&gt;AdvancedSubtensor</title>\n",
       "<path d=\"M717.632,-731.745C679.238,-721.197 614.243,-703.655 558,-690 528.213,-682.768 495.118,-675.344 466.944,-669.204\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"467.317,-665.703 456.803,-667.003 465.833,-672.544 467.317,-665.703\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"700.5\" y=\"-694.3\">0 TensorType(float32, matrix)</text>\n",
       "</g>\n",
       "<!-- Elemwise{log,no_inplace} -->\n",
       "<g class=\"node\" id=\"node50\"><title>Elemwise{log,no_inplace}</title>\n",
       "<ellipse cx=\"400\" cy=\"-566\" fill=\"#ffaabb\" rx=\"106.386\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"400\" y=\"-562.3\">Elemwise{log,no_inplace}</text>\n",
       "</g>\n",
       "<!-- AdvancedSubtensor&#45;&gt;Elemwise{log,no_inplace} -->\n",
       "<g class=\"edge\" id=\"edge52\"><title>AdvancedSubtensor-&gt;Elemwise{log,no_inplace}</title>\n",
       "<path d=\"M400,-635.597C400,-623.746 400,-607.817 400,-594.292\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"403.5,-594.084 400,-584.084 396.5,-594.084 403.5,-594.084\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"477\" y=\"-606.3\">TensorType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- Shape id=36 -->\n",
       "<g class=\"node\" id=\"node51\"><title>Shape id=36</title>\n",
       "<ellipse cx=\"486\" cy=\"-478\" fill=\"cyan\" rx=\"55.3436\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"486\" y=\"-474.3\">Shape id=36</text>\n",
       "</g>\n",
       "<!-- Elemwise{log,no_inplace}&#45;&gt;Shape id=36 -->\n",
       "<g class=\"edge\" id=\"edge53\"><title>Elemwise{log,no_inplace}-&gt;Shape id=36</title>\n",
       "<path d=\"M437.795,-549.123C446.875,-544.009 455.969,-537.643 463,-530 469.333,-523.115 474.157,-514.192 477.71,-505.739\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"481.06,-506.774 481.312,-496.182 474.51,-504.305 481.06,-506.774\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"551\" y=\"-518.3\">TensorType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- Sum{acc_dtype=float64} -->\n",
       "<g class=\"node\" id=\"node52\"><title>Sum{acc_dtype=float64}</title>\n",
       "<ellipse cx=\"306\" cy=\"-424\" fill=\"none\" rx=\"102.061\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306\" y=\"-420.3\">Sum{acc_dtype=float64}</text>\n",
       "</g>\n",
       "<!-- Elemwise{log,no_inplace}&#45;&gt;Sum{acc_dtype=float64} -->\n",
       "<g class=\"edge\" id=\"edge54\"><title>Elemwise{log,no_inplace}-&gt;Sum{acc_dtype=float64}</title>\n",
       "<path d=\"M332.608,-551.959C321.744,-546.869 311.836,-539.804 305,-530 289.176,-507.307 292.465,-474.62 297.766,-451.763\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"301.17,-452.582 300.283,-442.024 294.392,-450.831 301.17,-452.582\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-518.3\">TensorType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- Elemwise{Cast{float32}} -->\n",
       "<g class=\"node\" id=\"node53\"><title>Elemwise{Cast{float32}}</title>\n",
       "<ellipse cx=\"488\" cy=\"-370\" fill=\"#ffaabb\" rx=\"103.486\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-366.3\">Elemwise{Cast{float32}}</text>\n",
       "</g>\n",
       "<!-- Shape id=36&#45;&gt;Elemwise{Cast{float32}} -->\n",
       "<g class=\"edge\" id=\"edge55\"><title>Shape id=36-&gt;Elemwise{Cast{float32}}</title>\n",
       "<path d=\"M486.321,-459.969C486.634,-443.378 487.115,-417.883 487.482,-398.431\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"490.983,-398.405 487.673,-388.341 483.985,-398.273 490.983,-398.405\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"560\" y=\"-420.3\">TensorType(int64, vector)</text>\n",
       "</g>\n",
       "<!-- Elemwise{true_div,no_inplace} -->\n",
       "<g class=\"node\" id=\"node56\"><title>Elemwise{true_div,no_inplace}</title>\n",
       "<ellipse cx=\"360\" cy=\"-194\" fill=\"#ffaabb\" rx=\"124.184\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360\" y=\"-190.3\">Elemwise{true_div,no_inplace}</text>\n",
       "</g>\n",
       "<!-- Sum{acc_dtype=float64}&#45;&gt;Elemwise{true_div,no_inplace} -->\n",
       "<g class=\"edge\" id=\"edge58\"><title>Sum{acc_dtype=float64}-&gt;Elemwise{true_div,no_inplace}</title>\n",
       "<path d=\"M308.081,-405.77C310.77,-384.857 315.954,-348.592 323,-318 330.707,-284.535 342.531,-246.822 350.73,-222.081\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"354.141,-222.917 353.999,-212.323 347.504,-220.693 354.141,-222.917\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"404.5\" y=\"-322.3\">0 TensorType(float32, scalar)</text>\n",
       "</g>\n",
       "<!-- Subtensor{int64} id=39 -->\n",
       "<g class=\"node\" id=\"node54\"><title>Subtensor{int64} id=39</title>\n",
       "<ellipse cx=\"492\" cy=\"-282\" fill=\"#ffaaff\" rx=\"96.2874\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"492\" y=\"-278.3\">Subtensor{int64} id=39</text>\n",
       "</g>\n",
       "<!-- Elemwise{Cast{float32}}&#45;&gt;Subtensor{int64} id=39 -->\n",
       "<g class=\"edge\" id=\"edge56\"><title>Elemwise{Cast{float32}}-&gt;Subtensor{int64} id=39</title>\n",
       "<path d=\"M488.809,-351.597C489.361,-339.746 490.102,-323.817 490.731,-310.292\" fill=\"none\" stroke=\"blue\"/>\n",
       "<polygon fill=\"blue\" points=\"494.237,-310.236 491.205,-300.084 487.244,-309.911 494.237,-310.236\" stroke=\"blue\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"573.5\" y=\"-322.3\">0 TensorType(float32, vector)</text>\n",
       "</g>\n",
       "<!-- Subtensor{int64} id=39&#45;&gt;Elemwise{true_div,no_inplace} -->\n",
       "<g class=\"edge\" id=\"edge59\"><title>Subtensor{int64} id=39-&gt;Elemwise{true_div,no_inplace}</title>\n",
       "<path d=\"M466.861,-264.622C446.39,-251.284 417.272,-232.314 394.6,-217.542\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"396.237,-214.431 385.947,-211.905 392.415,-220.296 396.237,-214.431\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"519.5\" y=\"-234.3\">1 TensorType(float32, scalar)</text>\n",
       "</g>\n",
       "<!-- val=0 int64 id=53 -->\n",
       "<g class=\"node\" id=\"node55\"><title>val=0 int64 id=53</title>\n",
       "<polygon fill=\"green\" points=\"737.5,-388 622.5,-388 622.5,-352 737.5,-352 737.5,-388\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"680\" y=\"-366.3\">val=0 int64 id=53</text>\n",
       "</g>\n",
       "<!-- val=0 int64 id=53&#45;&gt;Subtensor{int64} id=39 -->\n",
       "<g class=\"edge\" id=\"edge57\"><title>val=0 int64 id=53-&gt;Subtensor{int64} id=39</title>\n",
       "<path d=\"M676.851,-351.853C673.888,-340.588 668.3,-326.426 658,-318 645.501,-307.775 613.252,-299.888 580.466,-294.212\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"580.734,-290.708 570.296,-292.521 579.586,-297.614 580.734,-290.708\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"689.5\" y=\"-322.3\">1 int64</text>\n",
       "</g>\n",
       "<!-- Elemwise{neg,no_inplace} -->\n",
       "<g class=\"node\" id=\"node57\"><title>Elemwise{neg,no_inplace}</title>\n",
       "<ellipse cx=\"360\" cy=\"-106\" fill=\"#ffaabb\" rx=\"108.31\" ry=\"18\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360\" y=\"-102.3\">Elemwise{neg,no_inplace}</text>\n",
       "</g>\n",
       "<!-- Elemwise{true_div,no_inplace}&#45;&gt;Elemwise{neg,no_inplace} -->\n",
       "<g class=\"edge\" id=\"edge60\"><title>Elemwise{true_div,no_inplace}-&gt;Elemwise{neg,no_inplace}</title>\n",
       "<path d=\"M360,-175.597C360,-163.746 360,-147.817 360,-134.292\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"363.5,-134.084 360,-124.084 356.5,-134.084 363.5,-134.084\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436\" y=\"-146.3\">TensorType(float32, scalar)</text>\n",
       "</g>\n",
       "<!-- TensorType(float32, scalar) id=56 -->\n",
       "<g class=\"node\" id=\"node58\"><title>TensorType(float32, scalar) id=56</title>\n",
       "<polygon fill=\"blue\" points=\"461.5,-36 258.5,-36 258.5,-0 461.5,-0 461.5,-36\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360\" y=\"-14.3\">TensorType(float32, scalar) id=56</text>\n",
       "</g>\n",
       "<!-- Elemwise{neg,no_inplace}&#45;&gt;TensorType(float32, scalar) id=56 -->\n",
       "<g class=\"edge\" id=\"edge61\"><title>Elemwise{neg,no_inplace}-&gt;TensorType(float32, scalar) id=56</title>\n",
       "<path d=\"M360,-87.5966C360,-75.7459 360,-59.8169 360,-46.2917\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"363.5,-46.084 360,-36.084 356.5,-46.084 363.5,-46.084\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436\" y=\"-58.3\">TensorType(float32, scalar)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# We have built a computation graph for computing the error_rate, predictions and cost\n",
    "#\n",
    "\n",
    "svgdotprint(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar('lrate',dtype='float32')\n",
    "momentum = theano.tensor.scalar('momentum',dtype='float32')\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in model_parameters]\n",
    "\n",
    "for p,g,v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v,v_new), (p, p_new)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_CW1, Elemwise{sub,no_inplace}.0),\n",
       " (CW1, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW2, Elemwise{sub,no_inplace}.0),\n",
       " (CW2, Elemwise{add,no_inplace}.0),\n",
       " (V_CB2, Elemwise{sub,no_inplace}.0),\n",
       " (CB2, Elemwise{add,no_inplace}.0),\n",
       " (V_FW3, Elemwise{sub,no_inplace}.0),\n",
       " (FW3, Elemwise{add,no_inplace}.0),\n",
       " (V_FB3, Elemwise{sub,no_inplace}.0),\n",
       " (FB3, Elemwise{add,no_inplace}.0),\n",
       " (V_FW4, Elemwise{sub,no_inplace}.0),\n",
       " (FW4, Elemwise{add,no_inplace}.0),\n",
       " (V_FB4, Elemwise{sub,no_inplace}.0),\n",
       " (FB4, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/wojciech/.theano/compiledir_Linux-3.19--generic-x86_64-with-LinuxMint-17.3-rosa-x86_64-3.4.3-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /home/wojciech/.theano/compiledir_Linux-3.19--generic-x86_64-with-LinuxMint-17.3-rosa-x86_64-3.4.3-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/wojciech/.theano/compiledir_Linux-3.19--generic-x86_64-with-LinuxMint-17.3-rosa-x86_64-3.4.3-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /home/wojciech/.theano/compiledir_Linux-3.19--generic-x86_64-with-LinuxMint-17.3-rosa-x86_64-3.4.3-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "#compile theano functions\n",
    "\n",
    "#each call to train step will make one SGD step\n",
    "train_step = theano.function([X,Y,lrate,momentum],[cost, error_rate, nll, weight_decay],updates=updates, allow_input_downcast=True)\n",
    "#each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X)!=Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs/num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utilities to save values of parameters and to load them\n",
    "\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow=False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "\n",
    "i=0\n",
    "e=0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 18300, batch loss 0.237297, batch nll 0.003664, batch error rate 0.000000%\n",
      "At minibatch 18400, batch loss 0.238116, batch nll 0.004780, batch error rate 0.000000%\n",
      "At minibatch 18500, batch loss 0.234849, batch nll 0.001840, batch error rate 0.000000%\n",
      "At minibatch 18600, batch loss 0.256284, batch nll 0.023589, batch error rate 0.000000%\n",
      "At minibatch 18700, batch loss 0.232886, batch nll 0.000504, batch error rate 0.000000%\n",
      "At minibatch 18800, batch loss 0.292655, batch nll 0.060583, batch error rate 4.000000%\n",
      "At minibatch 18900, batch loss 0.245106, batch nll 0.013369, batch error rate 0.000000%\n",
      "At minibatch 19000, batch loss 0.235317, batch nll 0.003873, batch error rate 0.000000%\n",
      "At minibatch 19100, batch loss 0.253919, batch nll 0.022774, batch error rate 0.000000%\n",
      "At minibatch 19200, batch loss 0.232947, batch nll 0.002112, batch error rate 0.000000%\n",
      "At minibatch 19300, batch loss 0.238583, batch nll 0.008052, batch error rate 0.000000%\n",
      "At minibatch 19400, batch loss 0.230646, batch nll 0.000398, batch error rate 0.000000%\n",
      "At minibatch 19500, batch loss 0.231949, batch nll 0.001961, batch error rate 0.000000%\n",
      "At minibatch 19600, batch loss 0.231802, batch nll 0.002104, batch error rate 0.000000%\n",
      "At minibatch 19700, batch loss 0.237823, batch nll 0.008416, batch error rate 0.000000%\n",
      "At minibatch 19800, batch loss 0.319851, batch nll 0.090747, batch error rate 8.000000%\n",
      "At minibatch 19900, batch loss 0.234102, batch nll 0.005274, batch error rate 0.000000%\n",
      "At minibatch 20000, batch loss 0.270371, batch nll 0.041833, batch error rate 4.000000%\n",
      "At minibatch 20100, batch loss 0.231864, batch nll 0.003635, batch error rate 0.000000%\n",
      "At minibatch 20200, batch loss 0.250163, batch nll 0.022227, batch error rate 0.000000%\n",
      "After epoch 11: valid_err_rate: 1.090000% currently going to do 17 epochs\n",
      "After epoch 11: averaged train_err_rate: 0.496248% averaged train nll: 0.019757 averaged train loss: 0.250548\n",
      "At minibatch 20300, batch loss 0.232527, batch nll 0.004856, batch error rate 0.000000%\n",
      "At minibatch 20400, batch loss 0.233556, batch nll 0.006142, batch error rate 0.000000%\n",
      "At minibatch 20500, batch loss 0.229149, batch nll 0.002015, batch error rate 0.000000%\n",
      "At minibatch 20600, batch loss 0.232604, batch nll 0.005738, batch error rate 0.000000%\n",
      "At minibatch 20700, batch loss 0.236227, batch nll 0.009643, batch error rate 0.000000%\n",
      "At minibatch 20800, batch loss 0.230871, batch nll 0.004569, batch error rate 0.000000%\n",
      "At minibatch 20900, batch loss 0.237229, batch nll 0.011203, batch error rate 0.000000%\n",
      "At minibatch 21000, batch loss 0.230750, batch nll 0.005003, batch error rate 0.000000%\n",
      "At minibatch 21100, batch loss 0.234401, batch nll 0.008907, batch error rate 0.000000%\n",
      "At minibatch 21200, batch loss 0.234053, batch nll 0.008831, batch error rate 0.000000%\n",
      "At minibatch 21300, batch loss 0.230628, batch nll 0.005669, batch error rate 0.000000%\n",
      "At minibatch 21400, batch loss 0.262235, batch nll 0.037547, batch error rate 4.000000%\n",
      "At minibatch 21500, batch loss 0.234849, batch nll 0.010415, batch error rate 0.000000%\n",
      "At minibatch 21600, batch loss 0.267334, batch nll 0.043145, batch error rate 0.000000%\n",
      "At minibatch 21700, batch loss 0.245093, batch nll 0.021181, batch error rate 0.000000%\n",
      "At minibatch 21800, batch loss 0.235114, batch nll 0.011432, batch error rate 0.000000%\n",
      "At minibatch 21900, batch loss 0.234111, batch nll 0.010691, batch error rate 0.000000%\n",
      "At minibatch 22000, batch loss 0.306683, batch nll 0.083497, batch error rate 4.000000%\n",
      "At minibatch 22100, batch loss 0.290164, batch nll 0.067233, batch error rate 4.000000%\n",
      "At minibatch 22200, batch loss 0.225355, batch nll 0.002683, batch error rate 0.000000%\n",
      "After epoch 12: valid_err_rate: 1.110000% currently going to do 17 epochs\n",
      "After epoch 12: averaged train_err_rate: 0.470235% averaged train nll: 0.018918 averaged train loss: 0.244090\n",
      "At minibatch 22300, batch loss 0.228401, batch nll 0.005982, batch error rate 0.000000%\n",
      "At minibatch 22400, batch loss 0.259973, batch nll 0.037807, batch error rate 0.000000%\n",
      "At minibatch 22500, batch loss 0.229632, batch nll 0.007678, batch error rate 0.000000%\n",
      "At minibatch 22600, batch loss 0.230390, batch nll 0.008674, batch error rate 0.000000%\n",
      "At minibatch 22700, batch loss 0.227542, batch nll 0.006078, batch error rate 0.000000%\n",
      "At minibatch 22800, batch loss 0.234767, batch nll 0.013545, batch error rate 0.000000%\n",
      "At minibatch 22900, batch loss 0.227505, batch nll 0.006537, batch error rate 0.000000%\n",
      "At minibatch 23000, batch loss 0.230853, batch nll 0.010112, batch error rate 0.000000%\n",
      "At minibatch 23100, batch loss 0.294529, batch nll 0.074021, batch error rate 4.000000%\n",
      "At minibatch 23200, batch loss 0.253676, batch nll 0.033395, batch error rate 0.000000%\n",
      "At minibatch 23300, batch loss 0.235742, batch nll 0.015701, batch error rate 0.000000%\n",
      "At minibatch 23400, batch loss 0.239826, batch nll 0.020017, batch error rate 0.000000%\n",
      "At minibatch 23500, batch loss 0.220192, batch nll 0.000596, batch error rate 0.000000%\n",
      "At minibatch 23600, batch loss 0.222277, batch nll 0.002910, batch error rate 0.000000%\n",
      "At minibatch 23700, batch loss 0.225982, batch nll 0.006862, batch error rate 0.000000%\n",
      "At minibatch 23800, batch loss 0.228192, batch nll 0.009300, batch error rate 0.000000%\n",
      "At minibatch 23900, batch loss 0.220197, batch nll 0.001525, batch error rate 0.000000%\n",
      "At minibatch 24000, batch loss 0.220776, batch nll 0.002319, batch error rate 0.000000%\n",
      "At minibatch 24100, batch loss 0.219868, batch nll 0.001635, batch error rate 0.000000%\n",
      "At minibatch 24200, batch loss 0.349576, batch nll 0.131568, batch error rate 4.000000%\n",
      "After epoch 13: valid_err_rate: 1.150000% currently going to do 17 epochs\n",
      "After epoch 13: averaged train_err_rate: 0.440220% averaged train nll: 0.018501 averaged train loss: 0.238723\n",
      "At minibatch 24300, batch loss 0.220681, batch nll 0.002898, batch error rate 0.000000%\n",
      "At minibatch 24400, batch loss 0.223631, batch nll 0.006085, batch error rate 0.000000%\n",
      "At minibatch 24500, batch loss 0.246143, batch nll 0.028804, batch error rate 0.000000%\n",
      "At minibatch 24600, batch loss 0.231569, batch nll 0.014439, batch error rate 0.000000%\n",
      "At minibatch 24700, batch loss 0.228417, batch nll 0.011490, batch error rate 0.000000%\n",
      "At minibatch 24800, batch loss 0.221500, batch nll 0.004786, batch error rate 0.000000%\n",
      "At minibatch 24900, batch loss 0.219839, batch nll 0.003350, batch error rate 0.000000%\n",
      "At minibatch 25000, batch loss 0.265020, batch nll 0.048733, batch error rate 4.000000%\n",
      "At minibatch 25100, batch loss 0.221237, batch nll 0.005172, batch error rate 0.000000%\n",
      "At minibatch 25200, batch loss 0.217880, batch nll 0.002026, batch error rate 0.000000%\n",
      "At minibatch 25300, batch loss 0.238902, batch nll 0.023262, batch error rate 0.000000%\n",
      "At minibatch 25400, batch loss 0.216918, batch nll 0.001502, batch error rate 0.000000%\n",
      "At minibatch 25500, batch loss 0.215706, batch nll 0.000500, batch error rate 0.000000%\n",
      "At minibatch 25600, batch loss 0.246914, batch nll 0.031912, batch error rate 0.000000%\n",
      "At minibatch 25700, batch loss 0.218110, batch nll 0.003304, batch error rate 0.000000%\n",
      "At minibatch 25800, batch loss 0.336281, batch nll 0.121677, batch error rate 4.000000%\n",
      "At minibatch 25900, batch loss 0.232201, batch nll 0.017792, batch error rate 0.000000%\n",
      "At minibatch 26000, batch loss 0.234246, batch nll 0.020045, batch error rate 0.000000%\n",
      "At minibatch 26100, batch loss 0.234840, batch nll 0.020835, batch error rate 0.000000%\n",
      "At minibatch 26200, batch loss 0.221871, batch nll 0.008061, batch error rate 0.000000%\n",
      "After epoch 14: valid_err_rate: 1.080000% currently going to do 22 epochs\n",
      "After epoch 14: averaged train_err_rate: 0.414207% averaged train nll: 0.017934 averaged train loss: 0.233731\n",
      "At minibatch 26300, batch loss 0.224496, batch nll 0.010890, batch error rate 0.000000%\n",
      "At minibatch 26400, batch loss 0.227639, batch nll 0.014225, batch error rate 0.000000%\n",
      "At minibatch 26500, batch loss 0.222307, batch nll 0.009102, batch error rate 0.000000%\n",
      "At minibatch 26600, batch loss 0.217680, batch nll 0.004667, batch error rate 0.000000%\n",
      "At minibatch 26700, batch loss 0.221777, batch nll 0.008972, batch error rate 0.000000%\n",
      "At minibatch 26800, batch loss 0.255523, batch nll 0.042916, batch error rate 0.000000%\n",
      "At minibatch 26900, batch loss 0.223079, batch nll 0.010669, batch error rate 0.000000%\n",
      "At minibatch 27000, batch loss 0.235660, batch nll 0.023418, batch error rate 0.000000%\n",
      "At minibatch 27100, batch loss 0.235893, batch nll 0.023839, batch error rate 0.000000%\n",
      "At minibatch 27200, batch loss 0.213416, batch nll 0.001557, batch error rate 0.000000%\n",
      "At minibatch 27300, batch loss 0.220433, batch nll 0.008771, batch error rate 0.000000%\n",
      "At minibatch 27400, batch loss 0.240567, batch nll 0.029082, batch error rate 0.000000%\n",
      "At minibatch 27500, batch loss 0.245597, batch nll 0.034301, batch error rate 0.000000%\n",
      "At minibatch 27600, batch loss 0.218283, batch nll 0.007183, batch error rate 0.000000%\n",
      "At minibatch 27700, batch loss 0.215424, batch nll 0.004500, batch error rate 0.000000%\n",
      "At minibatch 27800, batch loss 0.213615, batch nll 0.002875, batch error rate 0.000000%\n",
      "At minibatch 27900, batch loss 0.217112, batch nll 0.006555, batch error rate 0.000000%\n",
      "At minibatch 28000, batch loss 0.217343, batch nll 0.006969, batch error rate 0.000000%\n",
      "At minibatch 28100, batch loss 0.213039, batch nll 0.002846, batch error rate 0.000000%\n",
      "At minibatch 28200, batch loss 0.213469, batch nll 0.003465, batch error rate 0.000000%\n",
      "After epoch 15: valid_err_rate: 1.090000% currently going to do 22 epochs\n",
      "After epoch 15: averaged train_err_rate: 0.382191% averaged train nll: 0.017636 averaged train loss: 0.229446\n",
      "At minibatch 28300, batch loss 0.220359, batch nll 0.010533, batch error rate 0.000000%\n",
      "At minibatch 28400, batch loss 0.222414, batch nll 0.012767, batch error rate 0.000000%\n",
      "At minibatch 28500, batch loss 0.254185, batch nll 0.044719, batch error rate 0.000000%\n",
      "At minibatch 28600, batch loss 0.213004, batch nll 0.003707, batch error rate 0.000000%\n",
      "At minibatch 28700, batch loss 0.285366, batch nll 0.076246, batch error rate 4.000000%\n",
      "At minibatch 28800, batch loss 0.211430, batch nll 0.002487, batch error rate 0.000000%\n",
      "At minibatch 28900, batch loss 0.223688, batch nll 0.014922, batch error rate 0.000000%\n",
      "At minibatch 29000, batch loss 0.231747, batch nll 0.023154, batch error rate 0.000000%\n",
      "At minibatch 29100, batch loss 0.236776, batch nll 0.028365, batch error rate 0.000000%\n",
      "At minibatch 29200, batch loss 0.218550, batch nll 0.010318, batch error rate 0.000000%\n",
      "At minibatch 29300, batch loss 0.342933, batch nll 0.134891, batch error rate 4.000000%\n",
      "At minibatch 29400, batch loss 0.213724, batch nll 0.005852, batch error rate 0.000000%\n",
      "At minibatch 29500, batch loss 0.220041, batch nll 0.012345, batch error rate 0.000000%\n",
      "At minibatch 29600, batch loss 0.360090, batch nll 0.152559, batch error rate 8.000000%\n",
      "At minibatch 29700, batch loss 0.234384, batch nll 0.027022, batch error rate 0.000000%\n",
      "At minibatch 29800, batch loss 0.224777, batch nll 0.017578, batch error rate 0.000000%\n",
      "At minibatch 29900, batch loss 0.211773, batch nll 0.004733, batch error rate 0.000000%\n",
      "At minibatch 30000, batch loss 0.208951, batch nll 0.002080, batch error rate 0.000000%\n",
      "At minibatch 30100, batch loss 0.213526, batch nll 0.006810, batch error rate 0.000000%\n",
      "At minibatch 30200, batch loss 0.216348, batch nll 0.009801, batch error rate 0.000000%\n",
      "After epoch 16: valid_err_rate: 1.130000% currently going to do 22 epochs\n",
      "After epoch 16: averaged train_err_rate: 0.380190% averaged train nll: 0.017204 averaged train loss: 0.225393\n",
      "At minibatch 30300, batch loss 0.213922, batch nll 0.007540, batch error rate 0.000000%\n",
      "At minibatch 30400, batch loss 0.218760, batch nll 0.012545, batch error rate 0.000000%\n",
      "At minibatch 30500, batch loss 0.236267, batch nll 0.030207, batch error rate 0.000000%\n",
      "At minibatch 30600, batch loss 0.208922, batch nll 0.003018, batch error rate 0.000000%\n",
      "At minibatch 30700, batch loss 0.212303, batch nll 0.006559, batch error rate 0.000000%\n",
      "At minibatch 30800, batch loss 0.231039, batch nll 0.025463, batch error rate 0.000000%\n",
      "At minibatch 30900, batch loss 0.227415, batch nll 0.021994, batch error rate 0.000000%\n",
      "At minibatch 31000, batch loss 0.209858, batch nll 0.004609, batch error rate 0.000000%\n",
      "At minibatch 31100, batch loss 0.218519, batch nll 0.013427, batch error rate 0.000000%\n",
      "At minibatch 31200, batch loss 0.245732, batch nll 0.040812, batch error rate 4.000000%\n",
      "At minibatch 31300, batch loss 0.211753, batch nll 0.006989, batch error rate 0.000000%\n",
      "At minibatch 31400, batch loss 0.206810, batch nll 0.002214, batch error rate 0.000000%\n",
      "At minibatch 31500, batch loss 0.210704, batch nll 0.006253, batch error rate 0.000000%\n",
      "At minibatch 31600, batch loss 0.204670, batch nll 0.000378, batch error rate 0.000000%\n",
      "At minibatch 31700, batch loss 0.209123, batch nll 0.004984, batch error rate 0.000000%\n",
      "At minibatch 31800, batch loss 0.218153, batch nll 0.014167, batch error rate 0.000000%\n",
      "At minibatch 31900, batch loss 0.239699, batch nll 0.035863, batch error rate 4.000000%\n",
      "At minibatch 32000, batch loss 0.212964, batch nll 0.009280, batch error rate 0.000000%\n",
      "At minibatch 32100, batch loss 0.220296, batch nll 0.016752, batch error rate 0.000000%\n",
      "At minibatch 32200, batch loss 0.210014, batch nll 0.006636, batch error rate 0.000000%\n",
      "After epoch 17: valid_err_rate: 1.160000% currently going to do 22 epochs\n",
      "After epoch 17: averaged train_err_rate: 0.366183% averaged train nll: 0.017058 averaged train loss: 0.221946\n",
      "At minibatch 32300, batch loss 0.218039, batch nll 0.014804, batch error rate 0.000000%\n",
      "At minibatch 32400, batch loss 0.227096, batch nll 0.024002, batch error rate 0.000000%\n",
      "At minibatch 32500, batch loss 0.208104, batch nll 0.005173, batch error rate 0.000000%\n",
      "At minibatch 32600, batch loss 0.252104, batch nll 0.049337, batch error rate 0.000000%\n",
      "At minibatch 32700, batch loss 0.214109, batch nll 0.011481, batch error rate 0.000000%\n",
      "At minibatch 32800, batch loss 0.210420, batch nll 0.007946, batch error rate 0.000000%\n",
      "At minibatch 32900, batch loss 0.209492, batch nll 0.007157, batch error rate 0.000000%\n",
      "At minibatch 33000, batch loss 0.229557, batch nll 0.027369, batch error rate 0.000000%\n",
      "At minibatch 33100, batch loss 0.203801, batch nll 0.001762, batch error rate 0.000000%\n",
      "At minibatch 33200, batch loss 0.211584, batch nll 0.009684, batch error rate 0.000000%\n",
      "At minibatch 33300, batch loss 0.202984, batch nll 0.001230, batch error rate 0.000000%\n",
      "At minibatch 33400, batch loss 0.213145, batch nll 0.011526, batch error rate 0.000000%\n",
      "At minibatch 33500, batch loss 0.217181, batch nll 0.015709, batch error rate 0.000000%\n",
      "At minibatch 33600, batch loss 0.223682, batch nll 0.022350, batch error rate 0.000000%\n",
      "At minibatch 33700, batch loss 0.204068, batch nll 0.002876, batch error rate 0.000000%\n",
      "At minibatch 33800, batch loss 0.211126, batch nll 0.010074, batch error rate 0.000000%\n",
      "At minibatch 33900, batch loss 0.224199, batch nll 0.023289, batch error rate 0.000000%\n",
      "At minibatch 34000, batch loss 0.203322, batch nll 0.002556, batch error rate 0.000000%\n",
      "At minibatch 34100, batch loss 0.204122, batch nll 0.003496, batch error rate 0.000000%\n",
      "At minibatch 34200, batch loss 0.309933, batch nll 0.109449, batch error rate 4.000000%\n",
      "After epoch 18: valid_err_rate: 1.060000% currently going to do 28 epochs\n",
      "After epoch 18: averaged train_err_rate: 0.356178% averaged train nll: 0.016710 averaged train loss: 0.218575\n",
      "At minibatch 34300, batch loss 0.209999, batch nll 0.009657, batch error rate 0.000000%\n",
      "At minibatch 34400, batch loss 0.205013, batch nll 0.004813, batch error rate 0.000000%\n",
      "At minibatch 34500, batch loss 0.220771, batch nll 0.020711, batch error rate 0.000000%\n",
      "At minibatch 34600, batch loss 0.208701, batch nll 0.008781, batch error rate 0.000000%\n",
      "At minibatch 34700, batch loss 0.216254, batch nll 0.016466, batch error rate 0.000000%\n",
      "At minibatch 34800, batch loss 0.227468, batch nll 0.027822, batch error rate 0.000000%\n",
      "At minibatch 34900, batch loss 0.223738, batch nll 0.024219, batch error rate 0.000000%\n",
      "At minibatch 35000, batch loss 0.202521, batch nll 0.003138, batch error rate 0.000000%\n",
      "At minibatch 35100, batch loss 0.199908, batch nll 0.000656, batch error rate 0.000000%\n",
      "At minibatch 35200, batch loss 0.210574, batch nll 0.011452, batch error rate 0.000000%\n",
      "At minibatch 35300, batch loss 0.205080, batch nll 0.006087, batch error rate 0.000000%\n",
      "At minibatch 35400, batch loss 0.211912, batch nll 0.013059, batch error rate 0.000000%\n",
      "At minibatch 35500, batch loss 0.200242, batch nll 0.001517, batch error rate 0.000000%\n",
      "At minibatch 35600, batch loss 0.216478, batch nll 0.017885, batch error rate 0.000000%\n",
      "At minibatch 35700, batch loss 0.207483, batch nll 0.009031, batch error rate 0.000000%\n",
      "At minibatch 35800, batch loss 0.250696, batch nll 0.052378, batch error rate 0.000000%\n",
      "At minibatch 35900, batch loss 0.214556, batch nll 0.016365, batch error rate 0.000000%\n",
      "At minibatch 36000, batch loss 0.254793, batch nll 0.056731, batch error rate 4.000000%\n",
      "At minibatch 36100, batch loss 0.199294, batch nll 0.001369, batch error rate 0.000000%\n",
      "At minibatch 36200, batch loss 0.198427, batch nll 0.000632, batch error rate 0.000000%\n",
      "After epoch 19: valid_err_rate: 1.110000% currently going to do 28 epochs\n",
      "After epoch 19: averaged train_err_rate: 0.346173% averaged train nll: 0.016391 averaged train loss: 0.215471\n",
      "At minibatch 36300, batch loss 0.202002, batch nll 0.004334, batch error rate 0.000000%\n",
      "At minibatch 36400, batch loss 0.199174, batch nll 0.001634, batch error rate 0.000000%\n",
      "At minibatch 36500, batch loss 0.231682, batch nll 0.034266, batch error rate 4.000000%\n",
      "At minibatch 36600, batch loss 0.208785, batch nll 0.011494, batch error rate 0.000000%\n",
      "At minibatch 36700, batch loss 0.201152, batch nll 0.003990, batch error rate 0.000000%\n",
      "At minibatch 36800, batch loss 0.209431, batch nll 0.012404, batch error rate 0.000000%\n",
      "At minibatch 36900, batch loss 0.202351, batch nll 0.005454, batch error rate 0.000000%\n",
      "At minibatch 37000, batch loss 0.200576, batch nll 0.003800, batch error rate 0.000000%\n",
      "At minibatch 37100, batch loss 0.207972, batch nll 0.011332, batch error rate 0.000000%\n",
      "At minibatch 37200, batch loss 0.228408, batch nll 0.031888, batch error rate 0.000000%\n",
      "At minibatch 37300, batch loss 0.197296, batch nll 0.000905, batch error rate 0.000000%\n",
      "At minibatch 37400, batch loss 0.205576, batch nll 0.009312, batch error rate 0.000000%\n",
      "At minibatch 37500, batch loss 0.199074, batch nll 0.002933, batch error rate 0.000000%\n",
      "At minibatch 37600, batch loss 0.209081, batch nll 0.013071, batch error rate 0.000000%\n",
      "At minibatch 37700, batch loss 0.198514, batch nll 0.002633, batch error rate 0.000000%\n",
      "At minibatch 37800, batch loss 0.198254, batch nll 0.002487, batch error rate 0.000000%\n",
      "At minibatch 37900, batch loss 0.212712, batch nll 0.017073, batch error rate 0.000000%\n",
      "At minibatch 38000, batch loss 0.204149, batch nll 0.008626, batch error rate 0.000000%\n",
      "At minibatch 38100, batch loss 0.197428, batch nll 0.002020, batch error rate 0.000000%\n",
      "At minibatch 38200, batch loss 0.222032, batch nll 0.026753, batch error rate 0.000000%\n",
      "After epoch 20: valid_err_rate: 1.190000% currently going to do 28 epochs\n",
      "After epoch 20: averaged train_err_rate: 0.318159% averaged train nll: 0.016120 averaged train loss: 0.212603\n",
      "At minibatch 38300, batch loss 0.199721, batch nll 0.004558, batch error rate 0.000000%\n",
      "At minibatch 38400, batch loss 0.196400, batch nll 0.001354, batch error rate 0.000000%\n",
      "At minibatch 38500, batch loss 0.198891, batch nll 0.003960, batch error rate 0.000000%\n",
      "At minibatch 38600, batch loss 0.219827, batch nll 0.025013, batch error rate 0.000000%\n",
      "At minibatch 38700, batch loss 0.200672, batch nll 0.005979, batch error rate 0.000000%\n",
      "At minibatch 38800, batch loss 0.200926, batch nll 0.006347, batch error rate 0.000000%\n",
      "At minibatch 38900, batch loss 0.200801, batch nll 0.006341, batch error rate 0.000000%\n",
      "At minibatch 39000, batch loss 0.209526, batch nll 0.015183, batch error rate 0.000000%\n",
      "At minibatch 39100, batch loss 0.196425, batch nll 0.002202, batch error rate 0.000000%\n",
      "At minibatch 39200, batch loss 0.213253, batch nll 0.019144, batch error rate 0.000000%\n",
      "At minibatch 39300, batch loss 0.198608, batch nll 0.004607, batch error rate 0.000000%\n",
      "At minibatch 39400, batch loss 0.295510, batch nll 0.101618, batch error rate 4.000000%\n",
      "At minibatch 39500, batch loss 0.209939, batch nll 0.016180, batch error rate 0.000000%\n",
      "At minibatch 39600, batch loss 0.199576, batch nll 0.005926, batch error rate 0.000000%\n",
      "At minibatch 39700, batch loss 0.197455, batch nll 0.003916, batch error rate 0.000000%\n",
      "At minibatch 39800, batch loss 0.196859, batch nll 0.003440, batch error rate 0.000000%\n",
      "At minibatch 39900, batch loss 0.248876, batch nll 0.055583, batch error rate 4.000000%\n",
      "At minibatch 40000, batch loss 0.195299, batch nll 0.002128, batch error rate 0.000000%\n",
      "At minibatch 40100, batch loss 0.208231, batch nll 0.015177, batch error rate 0.000000%\n",
      "At minibatch 40200, batch loss 0.351388, batch nll 0.158451, batch error rate 4.000000%\n",
      "After epoch 21: valid_err_rate: 1.030000% currently going to do 32 epochs\n",
      "After epoch 21: averaged train_err_rate: 0.336168% averaged train nll: 0.016031 averaged train loss: 0.210105\n",
      "At minibatch 40300, batch loss 0.202207, batch nll 0.009387, batch error rate 0.000000%\n",
      "At minibatch 40400, batch loss 0.201449, batch nll 0.008737, batch error rate 0.000000%\n",
      "At minibatch 40500, batch loss 0.194294, batch nll 0.001689, batch error rate 0.000000%\n",
      "At minibatch 40600, batch loss 0.194883, batch nll 0.002386, batch error rate 0.000000%\n",
      "At minibatch 40700, batch loss 0.203811, batch nll 0.011416, batch error rate 0.000000%\n",
      "At minibatch 40800, batch loss 0.195786, batch nll 0.003493, batch error rate 0.000000%\n",
      "At minibatch 40900, batch loss 0.216460, batch nll 0.024286, batch error rate 0.000000%\n",
      "At minibatch 41000, batch loss 0.195671, batch nll 0.003610, batch error rate 0.000000%\n",
      "At minibatch 41100, batch loss 0.192443, batch nll 0.000493, batch error rate 0.000000%\n",
      "At minibatch 41200, batch loss 0.193591, batch nll 0.001750, batch error rate 0.000000%\n",
      "At minibatch 41300, batch loss 0.204664, batch nll 0.012932, batch error rate 0.000000%\n",
      "At minibatch 41400, batch loss 0.200877, batch nll 0.009255, batch error rate 0.000000%\n",
      "At minibatch 41500, batch loss 0.214512, batch nll 0.022997, batch error rate 0.000000%\n",
      "At minibatch 41600, batch loss 0.199789, batch nll 0.008388, batch error rate 0.000000%\n",
      "At minibatch 41700, batch loss 0.205346, batch nll 0.014057, batch error rate 0.000000%\n",
      "At minibatch 41800, batch loss 0.193376, batch nll 0.002201, batch error rate 0.000000%\n",
      "At minibatch 41900, batch loss 0.204111, batch nll 0.013048, batch error rate 0.000000%\n",
      "At minibatch 42000, batch loss 0.208905, batch nll 0.017950, batch error rate 0.000000%\n",
      "At minibatch 42100, batch loss 0.198682, batch nll 0.007830, batch error rate 0.000000%\n",
      "At minibatch 42200, batch loss 0.203677, batch nll 0.012936, batch error rate 0.000000%\n",
      "After epoch 22: valid_err_rate: 1.040000% currently going to do 32 epochs\n",
      "After epoch 22: averaged train_err_rate: 0.336168% averaged train nll: 0.015850 averaged train loss: 0.207654\n",
      "At minibatch 42300, batch loss 0.191940, batch nll 0.001301, batch error rate 0.000000%\n",
      "At minibatch 42400, batch loss 0.211729, batch nll 0.021189, batch error rate 0.000000%\n",
      "At minibatch 42500, batch loss 0.226444, batch nll 0.036006, batch error rate 4.000000%\n",
      "At minibatch 42600, batch loss 0.205093, batch nll 0.014754, batch error rate 0.000000%\n",
      "At minibatch 42700, batch loss 0.190891, batch nll 0.000661, batch error rate 0.000000%\n",
      "At minibatch 42800, batch loss 0.198557, batch nll 0.008431, batch error rate 0.000000%\n",
      "At minibatch 42900, batch loss 0.191560, batch nll 0.001533, batch error rate 0.000000%\n",
      "At minibatch 43000, batch loss 0.191401, batch nll 0.001480, batch error rate 0.000000%\n",
      "At minibatch 43100, batch loss 0.194785, batch nll 0.004974, batch error rate 0.000000%\n",
      "At minibatch 43200, batch loss 0.207925, batch nll 0.018221, batch error rate 0.000000%\n",
      "At minibatch 43300, batch loss 0.195061, batch nll 0.005460, batch error rate 0.000000%\n",
      "At minibatch 43400, batch loss 0.354549, batch nll 0.165049, batch error rate 4.000000%\n",
      "At minibatch 43500, batch loss 0.190428, batch nll 0.001038, batch error rate 0.000000%\n",
      "At minibatch 43600, batch loss 0.199793, batch nll 0.010507, batch error rate 0.000000%\n",
      "At minibatch 43700, batch loss 0.192855, batch nll 0.003670, batch error rate 0.000000%\n",
      "At minibatch 43800, batch loss 0.193694, batch nll 0.004609, batch error rate 0.000000%\n",
      "At minibatch 43900, batch loss 0.196393, batch nll 0.007411, batch error rate 0.000000%\n",
      "At minibatch 44000, batch loss 0.189253, batch nll 0.000373, batch error rate 0.000000%\n",
      "At minibatch 44100, batch loss 0.191887, batch nll 0.003104, batch error rate 0.000000%\n",
      "At minibatch 44200, batch loss 0.191987, batch nll 0.003305, batch error rate 0.000000%\n",
      "After epoch 23: valid_err_rate: 1.060000% currently going to do 32 epochs\n",
      "After epoch 23: averaged train_err_rate: 0.316158% averaged train nll: 0.015696 averaged train loss: 0.205371\n",
      "At minibatch 44300, batch loss 0.238160, batch nll 0.049582, batch error rate 4.000000%\n",
      "At minibatch 44400, batch loss 0.193082, batch nll 0.004597, batch error rate 0.000000%\n",
      "At minibatch 44500, batch loss 0.194579, batch nll 0.006187, batch error rate 0.000000%\n",
      "At minibatch 44600, batch loss 0.188811, batch nll 0.000520, batch error rate 0.000000%\n",
      "At minibatch 44700, batch loss 0.217292, batch nll 0.029102, batch error rate 0.000000%\n",
      "At minibatch 44800, batch loss 0.192504, batch nll 0.004405, batch error rate 0.000000%\n",
      "At minibatch 44900, batch loss 0.203600, batch nll 0.015599, batch error rate 0.000000%\n",
      "At minibatch 45000, batch loss 0.292546, batch nll 0.104633, batch error rate 4.000000%\n",
      "At minibatch 45100, batch loss 0.194917, batch nll 0.007113, batch error rate 0.000000%\n",
      "At minibatch 45200, batch loss 0.193064, batch nll 0.005356, batch error rate 0.000000%\n",
      "At minibatch 45300, batch loss 0.194398, batch nll 0.006788, batch error rate 0.000000%\n",
      "At minibatch 45400, batch loss 0.225425, batch nll 0.037912, batch error rate 0.000000%\n",
      "At minibatch 45500, batch loss 0.201746, batch nll 0.014331, batch error rate 0.000000%\n",
      "At minibatch 45600, batch loss 0.255072, batch nll 0.067756, batch error rate 4.000000%\n",
      "At minibatch 45700, batch loss 0.190457, batch nll 0.003238, batch error rate 0.000000%\n",
      "At minibatch 45800, batch loss 0.192190, batch nll 0.005063, batch error rate 0.000000%\n",
      "At minibatch 45900, batch loss 0.198328, batch nll 0.011295, batch error rate 0.000000%\n",
      "At minibatch 46000, batch loss 0.198105, batch nll 0.011160, batch error rate 0.000000%\n",
      "At minibatch 46100, batch loss 0.187874, batch nll 0.001027, batch error rate 0.000000%\n",
      "At minibatch 46200, batch loss 0.244272, batch nll 0.057525, batch error rate 4.000000%\n",
      "After epoch 24: valid_err_rate: 1.070000% currently going to do 32 epochs\n",
      "After epoch 24: averaged train_err_rate: 0.322161% averaged train nll: 0.015552 averaged train loss: 0.203230\n",
      "At minibatch 46300, batch loss 0.188277, batch nll 0.001629, batch error rate 0.000000%\n",
      "At minibatch 46400, batch loss 0.187715, batch nll 0.001154, batch error rate 0.000000%\n",
      "At minibatch 46500, batch loss 0.196298, batch nll 0.009830, batch error rate 0.000000%\n",
      "At minibatch 46600, batch loss 0.189963, batch nll 0.003587, batch error rate 0.000000%\n",
      "At minibatch 46700, batch loss 0.192930, batch nll 0.006650, batch error rate 0.000000%\n",
      "At minibatch 46800, batch loss 0.210658, batch nll 0.024472, batch error rate 0.000000%\n",
      "At minibatch 46900, batch loss 0.189933, batch nll 0.003834, batch error rate 0.000000%\n",
      "At minibatch 47000, batch loss 0.195716, batch nll 0.009711, batch error rate 0.000000%\n",
      "At minibatch 47100, batch loss 0.187995, batch nll 0.002077, batch error rate 0.000000%\n",
      "At minibatch 47200, batch loss 0.213569, batch nll 0.027747, batch error rate 0.000000%\n",
      "At minibatch 47300, batch loss 0.206042, batch nll 0.020312, batch error rate 0.000000%\n",
      "At minibatch 47400, batch loss 0.188201, batch nll 0.002565, batch error rate 0.000000%\n",
      "At minibatch 47500, batch loss 0.231999, batch nll 0.046450, batch error rate 4.000000%\n",
      "At minibatch 47600, batch loss 0.193922, batch nll 0.008466, batch error rate 0.000000%\n",
      "At minibatch 47700, batch loss 0.187119, batch nll 0.001751, batch error rate 0.000000%\n",
      "At minibatch 47800, batch loss 0.187715, batch nll 0.002431, batch error rate 0.000000%\n",
      "At minibatch 47900, batch loss 0.200099, batch nll 0.014909, batch error rate 0.000000%\n",
      "At minibatch 48000, batch loss 0.199881, batch nll 0.014774, batch error rate 0.000000%\n",
      "At minibatch 48100, batch loss 0.188423, batch nll 0.003413, batch error rate 0.000000%\n",
      "At minibatch 48200, batch loss 0.188387, batch nll 0.003469, batch error rate 0.000000%\n",
      "After epoch 25: valid_err_rate: 1.020000% currently going to do 38 epochs\n",
      "After epoch 25: averaged train_err_rate: 0.302151% averaged train nll: 0.015435 averaged train loss: 0.201231\n",
      "At minibatch 48300, batch loss 0.190391, batch nll 0.005567, batch error rate 0.000000%\n",
      "At minibatch 48400, batch loss 0.218646, batch nll 0.033909, batch error rate 0.000000%\n",
      "At minibatch 48500, batch loss 0.189533, batch nll 0.004883, batch error rate 0.000000%\n",
      "At minibatch 48600, batch loss 0.213120, batch nll 0.028558, batch error rate 0.000000%\n",
      "At minibatch 48700, batch loss 0.202083, batch nll 0.017616, batch error rate 0.000000%\n",
      "At minibatch 48800, batch loss 0.186150, batch nll 0.001767, batch error rate 0.000000%\n",
      "At minibatch 48900, batch loss 0.185626, batch nll 0.001326, batch error rate 0.000000%\n",
      "At minibatch 49000, batch loss 0.185356, batch nll 0.001150, batch error rate 0.000000%\n",
      "At minibatch 49100, batch loss 0.188835, batch nll 0.004712, batch error rate 0.000000%\n",
      "At minibatch 49200, batch loss 0.281719, batch nll 0.097681, batch error rate 4.000000%\n",
      "At minibatch 49300, batch loss 0.205078, batch nll 0.021118, batch error rate 0.000000%\n",
      "At minibatch 49400, batch loss 0.188931, batch nll 0.005059, batch error rate 0.000000%\n",
      "At minibatch 49500, batch loss 0.187379, batch nll 0.003593, batch error rate 0.000000%\n",
      "At minibatch 49600, batch loss 0.186495, batch nll 0.002798, batch error rate 0.000000%\n",
      "At minibatch 49700, batch loss 0.196898, batch nll 0.013288, batch error rate 0.000000%\n",
      "At minibatch 49800, batch loss 0.201666, batch nll 0.018149, batch error rate 0.000000%\n",
      "At minibatch 49900, batch loss 0.187887, batch nll 0.004459, batch error rate 0.000000%\n",
      "At minibatch 50000, batch loss 0.197493, batch nll 0.014145, batch error rate 0.000000%\n",
      "At minibatch 50100, batch loss 0.189265, batch nll 0.006004, batch error rate 0.000000%\n",
      "At minibatch 50200, batch loss 0.197894, batch nll 0.014714, batch error rate 0.000000%\n",
      "After epoch 26: valid_err_rate: 0.990000% currently going to do 40 epochs\n",
      "After epoch 26: averaged train_err_rate: 0.292146% averaged train nll: 0.015305 averaged train loss: 0.199317\n",
      "At minibatch 50300, batch loss 0.201305, batch nll 0.018203, batch error rate 0.000000%\n",
      "At minibatch 50400, batch loss 0.190495, batch nll 0.007474, batch error rate 0.000000%\n",
      "At minibatch 50500, batch loss 0.220801, batch nll 0.037862, batch error rate 0.000000%\n",
      "At minibatch 50600, batch loss 0.190803, batch nll 0.007947, batch error rate 0.000000%\n",
      "At minibatch 50700, batch loss 0.189787, batch nll 0.007010, batch error rate 0.000000%\n",
      "At minibatch 50800, batch loss 0.184165, batch nll 0.001473, batch error rate 0.000000%\n",
      "At minibatch 50900, batch loss 0.185427, batch nll 0.002817, batch error rate 0.000000%\n",
      "At minibatch 51000, batch loss 0.191797, batch nll 0.009273, batch error rate 0.000000%\n",
      "At minibatch 51100, batch loss 0.200858, batch nll 0.018417, batch error rate 0.000000%\n",
      "At minibatch 51200, batch loss 0.199389, batch nll 0.017034, batch error rate 0.000000%\n",
      "At minibatch 51300, batch loss 0.182769, batch nll 0.000500, batch error rate 0.000000%\n",
      "At minibatch 51400, batch loss 0.184923, batch nll 0.002735, batch error rate 0.000000%\n",
      "At minibatch 51500, batch loss 0.206863, batch nll 0.024758, batch error rate 0.000000%\n",
      "At minibatch 51600, batch loss 0.215455, batch nll 0.033434, batch error rate 0.000000%\n",
      "At minibatch 51700, batch loss 0.198163, batch nll 0.016226, batch error rate 0.000000%\n",
      "At minibatch 51800, batch loss 0.208712, batch nll 0.026855, batch error rate 0.000000%\n",
      "At minibatch 51900, batch loss 0.188096, batch nll 0.006317, batch error rate 0.000000%\n",
      "At minibatch 52000, batch loss 0.187341, batch nll 0.005642, batch error rate 0.000000%\n",
      "At minibatch 52100, batch loss 0.185525, batch nll 0.003905, batch error rate 0.000000%\n",
      "At minibatch 52200, batch loss 0.188014, batch nll 0.006471, batch error rate 0.000000%\n",
      "After epoch 27: valid_err_rate: 1.040000% currently going to do 40 epochs\n",
      "After epoch 27: averaged train_err_rate: 0.300150% averaged train nll: 0.015136 averaged train loss: 0.197467\n",
      "At minibatch 52300, batch loss 0.188177, batch nll 0.006707, batch error rate 0.000000%\n",
      "At minibatch 52400, batch loss 0.198486, batch nll 0.017105, batch error rate 0.000000%\n",
      "At minibatch 52500, batch loss 0.191991, batch nll 0.010693, batch error rate 0.000000%\n",
      "At minibatch 52600, batch loss 0.184125, batch nll 0.002906, batch error rate 0.000000%\n",
      "At minibatch 52700, batch loss 0.183585, batch nll 0.002444, batch error rate 0.000000%\n",
      "At minibatch 52800, batch loss 0.220969, batch nll 0.039912, batch error rate 0.000000%\n",
      "At minibatch 52900, batch loss 0.187855, batch nll 0.006873, batch error rate 0.000000%\n",
      "At minibatch 53000, batch loss 0.211261, batch nll 0.030352, batch error rate 0.000000%\n",
      "At minibatch 53100, batch loss 0.192939, batch nll 0.012109, batch error rate 0.000000%\n",
      "At minibatch 53200, batch loss 0.185352, batch nll 0.004595, batch error rate 0.000000%\n",
      "At minibatch 53300, batch loss 0.182409, batch nll 0.001732, batch error rate 0.000000%\n",
      "At minibatch 53400, batch loss 0.267640, batch nll 0.087046, batch error rate 4.000000%\n",
      "At minibatch 53500, batch loss 0.212536, batch nll 0.032018, batch error rate 0.000000%\n",
      "At minibatch 53600, batch loss 0.191055, batch nll 0.010618, batch error rate 0.000000%\n",
      "At minibatch 53700, batch loss 0.184057, batch nll 0.003697, batch error rate 0.000000%\n",
      "At minibatch 53800, batch loss 0.195813, batch nll 0.015528, batch error rate 0.000000%\n",
      "At minibatch 53900, batch loss 0.181184, batch nll 0.000972, batch error rate 0.000000%\n",
      "At minibatch 54000, batch loss 0.326155, batch nll 0.146022, batch error rate 4.000000%\n",
      "At minibatch 54100, batch loss 0.206237, batch nll 0.026181, batch error rate 0.000000%\n",
      "At minibatch 54200, batch loss 0.192806, batch nll 0.012830, batch error rate 0.000000%\n",
      "After epoch 28: valid_err_rate: 1.060000% currently going to do 40 epochs\n",
      "After epoch 28: averaged train_err_rate: 0.308154% averaged train nll: 0.015117 averaged train loss: 0.195844\n",
      "At minibatch 54300, batch loss 0.181751, batch nll 0.001850, batch error rate 0.000000%\n",
      "At minibatch 54400, batch loss 0.188990, batch nll 0.009162, batch error rate 0.000000%\n",
      "At minibatch 54500, batch loss 0.188520, batch nll 0.008770, batch error rate 0.000000%\n",
      "At minibatch 54600, batch loss 0.181869, batch nll 0.002194, batch error rate 0.000000%\n",
      "At minibatch 54700, batch loss 0.182795, batch nll 0.003191, batch error rate 0.000000%\n",
      "At minibatch 54800, batch loss 0.186555, batch nll 0.007027, batch error rate 0.000000%\n",
      "At minibatch 54900, batch loss 0.216107, batch nll 0.036654, batch error rate 4.000000%\n",
      "At minibatch 55000, batch loss 0.228564, batch nll 0.049187, batch error rate 4.000000%\n",
      "At minibatch 55100, batch loss 0.194202, batch nll 0.014900, batch error rate 0.000000%\n",
      "At minibatch 55200, batch loss 0.212875, batch nll 0.033639, batch error rate 0.000000%\n",
      "At minibatch 55300, batch loss 0.180353, batch nll 0.001189, batch error rate 0.000000%\n",
      "At minibatch 55400, batch loss 0.179865, batch nll 0.000779, batch error rate 0.000000%\n",
      "At minibatch 55500, batch loss 0.179528, batch nll 0.000515, batch error rate 0.000000%\n",
      "At minibatch 55600, batch loss 0.182766, batch nll 0.003827, batch error rate 0.000000%\n",
      "At minibatch 55700, batch loss 0.186074, batch nll 0.007210, batch error rate 0.000000%\n",
      "At minibatch 55800, batch loss 0.180324, batch nll 0.001535, batch error rate 0.000000%\n",
      "At minibatch 55900, batch loss 0.185077, batch nll 0.006357, batch error rate 0.000000%\n",
      "At minibatch 56000, batch loss 0.198281, batch nll 0.019631, batch error rate 0.000000%\n",
      "At minibatch 56100, batch loss 0.181180, batch nll 0.002606, batch error rate 0.000000%\n",
      "At minibatch 56200, batch loss 0.192555, batch nll 0.014053, batch error rate 0.000000%\n",
      "After epoch 29: valid_err_rate: 1.100000% currently going to do 40 epochs\n",
      "After epoch 29: averaged train_err_rate: 0.270135% averaged train nll: 0.014936 averaged train loss: 0.194146\n",
      "At minibatch 56300, batch loss 0.191500, batch nll 0.013074, batch error rate 0.000000%\n",
      "At minibatch 56400, batch loss 0.189102, batch nll 0.010747, batch error rate 0.000000%\n",
      "At minibatch 56500, batch loss 0.186551, batch nll 0.008267, batch error rate 0.000000%\n",
      "At minibatch 56600, batch loss 0.249940, batch nll 0.071735, batch error rate 4.000000%\n",
      "At minibatch 56700, batch loss 0.180823, batch nll 0.002685, batch error rate 0.000000%\n",
      "At minibatch 56800, batch loss 0.182449, batch nll 0.004384, batch error rate 0.000000%\n",
      "At minibatch 56900, batch loss 0.182619, batch nll 0.004626, batch error rate 0.000000%\n",
      "At minibatch 57000, batch loss 0.183713, batch nll 0.005799, batch error rate 0.000000%\n",
      "At minibatch 57100, batch loss 0.185905, batch nll 0.008061, batch error rate 0.000000%\n",
      "At minibatch 57200, batch loss 0.186764, batch nll 0.008993, batch error rate 0.000000%\n",
      "At minibatch 57300, batch loss 0.178596, batch nll 0.000891, batch error rate 0.000000%\n",
      "At minibatch 57400, batch loss 0.179398, batch nll 0.001760, batch error rate 0.000000%\n",
      "At minibatch 57500, batch loss 0.184919, batch nll 0.007352, batch error rate 0.000000%\n",
      "At minibatch 57600, batch loss 0.185624, batch nll 0.008132, batch error rate 0.000000%\n",
      "At minibatch 57700, batch loss 0.193123, batch nll 0.015704, batch error rate 0.000000%\n",
      "At minibatch 57800, batch loss 0.204689, batch nll 0.027336, batch error rate 0.000000%\n",
      "At minibatch 57900, batch loss 0.182094, batch nll 0.004817, batch error rate 0.000000%\n",
      "At minibatch 58000, batch loss 0.201599, batch nll 0.024386, batch error rate 0.000000%\n",
      "At minibatch 58100, batch loss 0.181866, batch nll 0.004716, batch error rate 0.000000%\n",
      "At minibatch 58200, batch loss 0.179490, batch nll 0.002410, batch error rate 0.000000%\n",
      "After epoch 30: valid_err_rate: 1.050000% currently going to do 40 epochs\n",
      "After epoch 30: averaged train_err_rate: 0.274137% averaged train nll: 0.014823 averaged train loss: 0.192580\n",
      "At minibatch 58300, batch loss 0.180648, batch nll 0.003633, batch error rate 0.000000%\n",
      "At minibatch 58400, batch loss 0.179387, batch nll 0.002437, batch error rate 0.000000%\n",
      "At minibatch 58500, batch loss 0.200071, batch nll 0.023190, batch error rate 0.000000%\n",
      "At minibatch 58600, batch loss 0.214254, batch nll 0.037445, batch error rate 0.000000%\n",
      "At minibatch 58700, batch loss 0.185048, batch nll 0.008304, batch error rate 0.000000%\n",
      "At minibatch 58800, batch loss 0.180548, batch nll 0.003875, batch error rate 0.000000%\n",
      "At minibatch 58900, batch loss 0.207519, batch nll 0.030912, batch error rate 0.000000%\n",
      "At minibatch 59000, batch loss 0.193995, batch nll 0.017458, batch error rate 0.000000%\n",
      "At minibatch 59100, batch loss 0.186363, batch nll 0.009898, batch error rate 0.000000%\n",
      "At minibatch 59200, batch loss 0.179599, batch nll 0.003196, batch error rate 0.000000%\n",
      "At minibatch 59300, batch loss 0.181388, batch nll 0.005053, batch error rate 0.000000%\n",
      "At minibatch 59400, batch loss 0.185436, batch nll 0.009170, batch error rate 0.000000%\n",
      "At minibatch 59500, batch loss 0.211647, batch nll 0.035449, batch error rate 0.000000%\n",
      "At minibatch 59600, batch loss 0.178970, batch nll 0.002837, batch error rate 0.000000%\n",
      "At minibatch 59700, batch loss 0.179516, batch nll 0.003450, batch error rate 0.000000%\n",
      "At minibatch 59800, batch loss 0.177515, batch nll 0.001516, batch error rate 0.000000%\n",
      "At minibatch 59900, batch loss 0.188093, batch nll 0.012164, batch error rate 0.000000%\n",
      "At minibatch 60000, batch loss 0.201936, batch nll 0.026080, batch error rate 0.000000%\n",
      "At minibatch 60100, batch loss 0.184569, batch nll 0.008781, batch error rate 0.000000%\n",
      "At minibatch 60200, batch loss 0.178372, batch nll 0.002647, batch error rate 0.000000%\n",
      "After epoch 31: valid_err_rate: 1.060000% currently going to do 40 epochs\n",
      "After epoch 31: averaged train_err_rate: 0.260130% averaged train nll: 0.014726 averaged train loss: 0.191106\n",
      "At minibatch 60300, batch loss 0.220546, batch nll 0.044893, batch error rate 4.000000%\n",
      "At minibatch 60400, batch loss 0.178394, batch nll 0.002806, batch error rate 0.000000%\n",
      "At minibatch 60500, batch loss 0.180235, batch nll 0.004714, batch error rate 0.000000%\n",
      "At minibatch 60600, batch loss 0.210674, batch nll 0.035219, batch error rate 0.000000%\n",
      "At minibatch 60700, batch loss 0.178591, batch nll 0.003199, batch error rate 0.000000%\n",
      "At minibatch 60800, batch loss 0.183552, batch nll 0.008228, batch error rate 0.000000%\n",
      "At minibatch 60900, batch loss 0.177801, batch nll 0.002537, batch error rate 0.000000%\n",
      "At minibatch 61000, batch loss 0.177267, batch nll 0.002068, batch error rate 0.000000%\n",
      "At minibatch 61100, batch loss 0.198989, batch nll 0.023850, batch error rate 0.000000%\n",
      "At minibatch 61200, batch loss 0.190320, batch nll 0.015248, batch error rate 0.000000%\n",
      "At minibatch 61300, batch loss 0.330041, batch nll 0.155030, batch error rate 4.000000%\n",
      "At minibatch 61400, batch loss 0.203585, batch nll 0.028644, batch error rate 0.000000%\n",
      "At minibatch 61500, batch loss 0.176564, batch nll 0.001688, batch error rate 0.000000%\n",
      "At minibatch 61600, batch loss 0.185463, batch nll 0.010648, batch error rate 0.000000%\n",
      "At minibatch 61700, batch loss 0.176629, batch nll 0.001881, batch error rate 0.000000%\n",
      "At minibatch 61800, batch loss 0.179978, batch nll 0.005295, batch error rate 0.000000%\n",
      "At minibatch 61900, batch loss 0.180314, batch nll 0.005697, batch error rate 0.000000%\n",
      "At minibatch 62000, batch loss 0.175654, batch nll 0.001098, batch error rate 0.000000%\n",
      "At minibatch 62100, batch loss 0.179126, batch nll 0.004632, batch error rate 0.000000%\n",
      "At minibatch 62200, batch loss 0.179585, batch nll 0.005157, batch error rate 0.000000%\n",
      "After epoch 32: valid_err_rate: 1.040000% currently going to do 40 epochs\n",
      "After epoch 32: averaged train_err_rate: 0.276138% averaged train nll: 0.014677 averaged train loss: 0.189726\n",
      "At minibatch 62300, batch loss 0.177598, batch nll 0.003233, batch error rate 0.000000%\n",
      "At minibatch 62400, batch loss 0.179087, batch nll 0.004786, batch error rate 0.000000%\n",
      "At minibatch 62500, batch loss 0.175744, batch nll 0.001504, batch error rate 0.000000%\n",
      "At minibatch 62600, batch loss 0.175103, batch nll 0.000924, batch error rate 0.000000%\n",
      "At minibatch 62700, batch loss 0.178229, batch nll 0.004112, batch error rate 0.000000%\n",
      "At minibatch 62800, batch loss 0.177273, batch nll 0.003219, batch error rate 0.000000%\n",
      "At minibatch 62900, batch loss 0.178377, batch nll 0.004387, batch error rate 0.000000%\n",
      "At minibatch 63000, batch loss 0.185935, batch nll 0.012005, batch error rate 0.000000%\n",
      "At minibatch 63100, batch loss 0.176955, batch nll 0.003085, batch error rate 0.000000%\n",
      "At minibatch 63200, batch loss 0.180662, batch nll 0.006855, batch error rate 0.000000%\n",
      "At minibatch 63300, batch loss 0.176295, batch nll 0.002552, batch error rate 0.000000%\n",
      "At minibatch 63400, batch loss 0.175807, batch nll 0.002130, batch error rate 0.000000%\n",
      "At minibatch 63500, batch loss 0.275917, batch nll 0.102303, batch error rate 8.000000%\n",
      "At minibatch 63600, batch loss 0.189294, batch nll 0.015740, batch error rate 0.000000%\n",
      "At minibatch 63700, batch loss 0.176684, batch nll 0.003189, batch error rate 0.000000%\n",
      "At minibatch 63800, batch loss 0.212162, batch nll 0.038729, batch error rate 4.000000%\n",
      "At minibatch 63900, batch loss 0.206058, batch nll 0.032684, batch error rate 0.000000%\n",
      "At minibatch 64000, batch loss 0.178529, batch nll 0.005225, batch error rate 0.000000%\n",
      "At minibatch 64100, batch loss 0.173695, batch nll 0.000446, batch error rate 0.000000%\n",
      "At minibatch 64200, batch loss 0.184887, batch nll 0.011700, batch error rate 0.000000%\n",
      "After epoch 33: valid_err_rate: 1.010000% currently going to do 40 epochs\n",
      "After epoch 33: averaged train_err_rate: 0.286143% averaged train nll: 0.014597 averaged train loss: 0.188382\n",
      "At minibatch 64300, batch loss 0.174259, batch nll 0.001133, batch error rate 0.000000%\n",
      "At minibatch 64400, batch loss 0.179327, batch nll 0.006258, batch error rate 0.000000%\n",
      "At minibatch 64500, batch loss 0.176943, batch nll 0.003932, batch error rate 0.000000%\n",
      "At minibatch 64600, batch loss 0.178024, batch nll 0.005074, batch error rate 0.000000%\n",
      "At minibatch 64700, batch loss 0.214969, batch nll 0.042081, batch error rate 0.000000%\n",
      "At minibatch 64800, batch loss 0.237053, batch nll 0.064225, batch error rate 4.000000%\n",
      "At minibatch 64900, batch loss 0.180954, batch nll 0.008192, batch error rate 0.000000%\n",
      "At minibatch 65000, batch loss 0.197517, batch nll 0.024813, batch error rate 0.000000%\n",
      "At minibatch 65100, batch loss 0.176022, batch nll 0.003376, batch error rate 0.000000%\n",
      "At minibatch 65200, batch loss 0.174641, batch nll 0.002057, batch error rate 0.000000%\n",
      "At minibatch 65300, batch loss 0.195603, batch nll 0.023077, batch error rate 0.000000%\n",
      "At minibatch 65400, batch loss 0.178874, batch nll 0.006405, batch error rate 0.000000%\n",
      "At minibatch 65500, batch loss 0.176146, batch nll 0.003733, batch error rate 0.000000%\n",
      "At minibatch 65600, batch loss 0.182819, batch nll 0.010467, batch error rate 0.000000%\n",
      "At minibatch 65700, batch loss 0.175310, batch nll 0.003020, batch error rate 0.000000%\n",
      "At minibatch 65800, batch loss 0.178844, batch nll 0.006619, batch error rate 0.000000%\n",
      "At minibatch 65900, batch loss 0.177006, batch nll 0.004838, batch error rate 0.000000%\n",
      "At minibatch 66000, batch loss 0.174198, batch nll 0.002089, batch error rate 0.000000%\n",
      "At minibatch 66100, batch loss 0.191959, batch nll 0.019908, batch error rate 0.000000%\n",
      "At minibatch 66200, batch loss 0.203131, batch nll 0.031139, batch error rate 0.000000%\n",
      "After epoch 34: valid_err_rate: 1.010000% currently going to do 40 epochs\n",
      "After epoch 34: averaged train_err_rate: 0.284142% averaged train nll: 0.014514 averaged train loss: 0.187083\n",
      "At minibatch 66300, batch loss 0.183641, batch nll 0.011704, batch error rate 0.000000%\n",
      "At minibatch 66400, batch loss 0.179719, batch nll 0.007844, batch error rate 0.000000%\n",
      "At minibatch 66500, batch loss 0.172089, batch nll 0.000271, batch error rate 0.000000%\n",
      "At minibatch 66600, batch loss 0.185816, batch nll 0.014055, batch error rate 0.000000%\n",
      "At minibatch 66700, batch loss 0.175318, batch nll 0.003614, batch error rate 0.000000%\n",
      "At minibatch 66800, batch loss 0.172503, batch nll 0.000859, batch error rate 0.000000%\n",
      "At minibatch 66900, batch loss 0.172184, batch nll 0.000600, batch error rate 0.000000%\n",
      "At minibatch 67000, batch loss 0.173651, batch nll 0.002121, batch error rate 0.000000%\n",
      "At minibatch 67100, batch loss 0.194935, batch nll 0.023457, batch error rate 0.000000%\n",
      "At minibatch 67200, batch loss 0.176442, batch nll 0.005020, batch error rate 0.000000%\n",
      "At minibatch 67300, batch loss 0.174249, batch nll 0.002884, batch error rate 0.000000%\n",
      "At minibatch 67400, batch loss 0.175058, batch nll 0.003748, batch error rate 0.000000%\n",
      "At minibatch 67500, batch loss 0.172687, batch nll 0.001438, batch error rate 0.000000%\n",
      "At minibatch 67600, batch loss 0.172271, batch nll 0.001076, batch error rate 0.000000%\n",
      "At minibatch 67700, batch loss 0.174600, batch nll 0.003463, batch error rate 0.000000%\n",
      "At minibatch 67800, batch loss 0.181692, batch nll 0.010614, batch error rate 0.000000%\n",
      "At minibatch 67900, batch loss 0.174010, batch nll 0.002986, batch error rate 0.000000%\n",
      "At minibatch 68000, batch loss 0.222072, batch nll 0.051110, batch error rate 4.000000%\n",
      "At minibatch 68100, batch loss 0.173779, batch nll 0.002878, batch error rate 0.000000%\n",
      "At minibatch 68200, batch loss 0.176417, batch nll 0.005571, batch error rate 0.000000%\n",
      "After epoch 35: valid_err_rate: 1.040000% currently going to do 40 epochs\n",
      "After epoch 35: averaged train_err_rate: 0.268134% averaged train nll: 0.014467 averaged train loss: 0.185868\n",
      "At minibatch 68300, batch loss 0.205269, batch nll 0.034481, batch error rate 4.000000%\n",
      "At minibatch 68400, batch loss 0.175492, batch nll 0.004762, batch error rate 0.000000%\n",
      "At minibatch 68500, batch loss 0.199043, batch nll 0.028368, batch error rate 0.000000%\n",
      "At minibatch 68600, batch loss 0.184119, batch nll 0.013499, batch error rate 0.000000%\n",
      "At minibatch 68700, batch loss 0.175759, batch nll 0.005198, batch error rate 0.000000%\n",
      "At minibatch 68800, batch loss 0.172516, batch nll 0.002012, batch error rate 0.000000%\n",
      "At minibatch 68900, batch loss 0.178156, batch nll 0.007708, batch error rate 0.000000%\n",
      "At minibatch 69000, batch loss 0.183188, batch nll 0.012795, batch error rate 0.000000%\n",
      "At minibatch 69100, batch loss 0.178346, batch nll 0.008009, batch error rate 0.000000%\n",
      "At minibatch 69200, batch loss 0.211308, batch nll 0.041022, batch error rate 0.000000%\n",
      "At minibatch 69300, batch loss 0.187360, batch nll 0.017133, batch error rate 0.000000%\n",
      "At minibatch 69400, batch loss 0.189846, batch nll 0.019673, batch error rate 0.000000%\n",
      "At minibatch 69500, batch loss 0.190624, batch nll 0.020504, batch error rate 0.000000%\n",
      "At minibatch 69600, batch loss 0.190836, batch nll 0.020769, batch error rate 0.000000%\n",
      "At minibatch 69700, batch loss 0.212513, batch nll 0.042500, batch error rate 0.000000%\n",
      "At minibatch 69800, batch loss 0.175771, batch nll 0.005812, batch error rate 0.000000%\n",
      "At minibatch 69900, batch loss 0.174668, batch nll 0.004763, batch error rate 0.000000%\n",
      "At minibatch 70000, batch loss 0.177869, batch nll 0.008018, batch error rate 0.000000%\n",
      "At minibatch 70100, batch loss 0.178004, batch nll 0.008201, batch error rate 0.000000%\n",
      "At minibatch 70200, batch loss 0.174674, batch nll 0.004927, batch error rate 0.000000%\n",
      "After epoch 36: valid_err_rate: 1.040000% currently going to do 40 epochs\n",
      "After epoch 36: averaged train_err_rate: 0.252126% averaged train nll: 0.014392 averaged train loss: 0.184661\n",
      "At minibatch 70300, batch loss 0.172462, batch nll 0.002773, batch error rate 0.000000%\n",
      "At minibatch 70400, batch loss 0.199071, batch nll 0.029435, batch error rate 0.000000%\n",
      "At minibatch 70500, batch loss 0.194560, batch nll 0.024978, batch error rate 0.000000%\n",
      "At minibatch 70600, batch loss 0.173532, batch nll 0.003997, batch error rate 0.000000%\n",
      "At minibatch 70700, batch loss 0.172500, batch nll 0.003013, batch error rate 0.000000%\n",
      "At minibatch 70800, batch loss 0.181427, batch nll 0.011995, batch error rate 0.000000%\n",
      "At minibatch 70900, batch loss 0.173793, batch nll 0.004416, batch error rate 0.000000%\n",
      "At minibatch 71000, batch loss 0.172655, batch nll 0.003335, batch error rate 0.000000%\n",
      "At minibatch 71100, batch loss 0.218301, batch nll 0.049030, batch error rate 4.000000%\n",
      "At minibatch 71200, batch loss 0.177995, batch nll 0.008776, batch error rate 0.000000%\n",
      "At minibatch 71300, batch loss 0.176652, batch nll 0.007485, batch error rate 0.000000%\n",
      "At minibatch 71400, batch loss 0.177791, batch nll 0.008682, batch error rate 0.000000%\n",
      "At minibatch 71500, batch loss 0.182526, batch nll 0.013468, batch error rate 0.000000%\n",
      "At minibatch 71600, batch loss 0.170354, batch nll 0.001348, batch error rate 0.000000%\n",
      "At minibatch 71700, batch loss 0.201804, batch nll 0.032849, batch error rate 4.000000%\n",
      "At minibatch 71800, batch loss 0.171782, batch nll 0.002884, batch error rate 0.000000%\n",
      "At minibatch 71900, batch loss 0.170333, batch nll 0.001486, batch error rate 0.000000%\n",
      "At minibatch 72000, batch loss 0.176930, batch nll 0.008138, batch error rate 0.000000%\n",
      "At minibatch 72100, batch loss 0.244778, batch nll 0.076044, batch error rate 4.000000%\n",
      "At minibatch 72200, batch loss 0.170125, batch nll 0.001442, batch error rate 0.000000%\n",
      "After epoch 37: valid_err_rate: 1.050000% currently going to do 40 epochs\n",
      "After epoch 37: averaged train_err_rate: 0.268134% averaged train nll: 0.014312 averaged train loss: 0.183511\n",
      "At minibatch 72300, batch loss 0.186137, batch nll 0.017505, batch error rate 0.000000%\n",
      "At minibatch 72400, batch loss 0.184013, batch nll 0.015435, batch error rate 0.000000%\n",
      "At minibatch 72500, batch loss 0.177083, batch nll 0.008555, batch error rate 0.000000%\n",
      "At minibatch 72600, batch loss 0.183858, batch nll 0.015377, batch error rate 0.000000%\n",
      "At minibatch 72700, batch loss 0.182125, batch nll 0.013697, batch error rate 0.000000%\n",
      "At minibatch 72800, batch loss 0.172974, batch nll 0.004599, batch error rate 0.000000%\n",
      "At minibatch 72900, batch loss 0.171312, batch nll 0.002989, batch error rate 0.000000%\n",
      "At minibatch 73000, batch loss 0.168786, batch nll 0.000515, batch error rate 0.000000%\n",
      "At minibatch 73100, batch loss 0.187632, batch nll 0.019411, batch error rate 0.000000%\n",
      "At minibatch 73200, batch loss 0.169054, batch nll 0.000890, batch error rate 0.000000%\n",
      "At minibatch 73300, batch loss 0.181993, batch nll 0.013879, batch error rate 0.000000%\n",
      "At minibatch 73400, batch loss 0.174235, batch nll 0.006171, batch error rate 0.000000%\n",
      "At minibatch 73500, batch loss 0.168851, batch nll 0.000841, batch error rate 0.000000%\n",
      "At minibatch 73600, batch loss 0.191424, batch nll 0.023472, batch error rate 0.000000%\n",
      "At minibatch 73700, batch loss 0.169906, batch nll 0.002002, batch error rate 0.000000%\n",
      "At minibatch 73800, batch loss 0.173280, batch nll 0.005429, batch error rate 0.000000%\n",
      "At minibatch 73900, batch loss 0.174452, batch nll 0.006650, batch error rate 0.000000%\n",
      "At minibatch 74000, batch loss 0.174289, batch nll 0.006537, batch error rate 0.000000%\n",
      "At minibatch 74100, batch loss 0.177957, batch nll 0.010253, batch error rate 0.000000%\n",
      "At minibatch 74200, batch loss 0.190018, batch nll 0.022359, batch error rate 0.000000%\n",
      "After epoch 38: valid_err_rate: 1.040000% currently going to do 40 epochs\n",
      "After epoch 38: averaged train_err_rate: 0.268134% averaged train nll: 0.014227 averaged train loss: 0.182377\n",
      "At minibatch 74300, batch loss 0.174656, batch nll 0.007047, batch error rate 0.000000%\n",
      "At minibatch 74400, batch loss 0.175528, batch nll 0.007971, batch error rate 0.000000%\n",
      "At minibatch 74500, batch loss 0.170164, batch nll 0.002656, batch error rate 0.000000%\n",
      "At minibatch 74600, batch loss 0.179609, batch nll 0.012151, batch error rate 0.000000%\n",
      "At minibatch 74700, batch loss 0.179310, batch nll 0.011898, batch error rate 0.000000%\n",
      "At minibatch 74800, batch loss 0.169774, batch nll 0.002416, batch error rate 0.000000%\n",
      "At minibatch 74900, batch loss 0.175303, batch nll 0.007996, batch error rate 0.000000%\n",
      "At minibatch 75000, batch loss 0.170868, batch nll 0.003612, batch error rate 0.000000%\n",
      "At minibatch 75100, batch loss 0.171433, batch nll 0.004225, batch error rate 0.000000%\n",
      "At minibatch 75200, batch loss 0.182091, batch nll 0.014930, batch error rate 0.000000%\n",
      "At minibatch 75300, batch loss 0.169498, batch nll 0.002385, batch error rate 0.000000%\n",
      "At minibatch 75400, batch loss 0.172015, batch nll 0.004948, batch error rate 0.000000%\n",
      "At minibatch 75500, batch loss 0.167646, batch nll 0.000625, batch error rate 0.000000%\n",
      "At minibatch 75600, batch loss 0.173709, batch nll 0.006735, batch error rate 0.000000%\n",
      "At minibatch 75700, batch loss 0.167968, batch nll 0.001043, batch error rate 0.000000%\n",
      "At minibatch 75800, batch loss 0.170200, batch nll 0.003323, batch error rate 0.000000%\n",
      "At minibatch 75900, batch loss 0.171901, batch nll 0.005075, batch error rate 0.000000%\n",
      "At minibatch 76000, batch loss 0.180592, batch nll 0.013819, batch error rate 0.000000%\n",
      "At minibatch 76100, batch loss 0.275927, batch nll 0.109203, batch error rate 4.000000%\n",
      "At minibatch 76200, batch loss 0.174751, batch nll 0.008075, batch error rate 0.000000%\n",
      "After epoch 39: valid_err_rate: 1.040000% currently going to do 40 epochs\n",
      "After epoch 39: averaged train_err_rate: 0.246123% averaged train nll: 0.014169 averaged train loss: 0.181318\n",
      "At minibatch 76300, batch loss 0.170209, batch nll 0.003581, batch error rate 0.000000%\n",
      "At minibatch 76400, batch loss 0.173915, batch nll 0.007339, batch error rate 0.000000%\n",
      "At minibatch 76500, batch loss 0.239978, batch nll 0.073447, batch error rate 4.000000%\n",
      "At minibatch 76600, batch loss 0.171357, batch nll 0.004873, batch error rate 0.000000%\n",
      "At minibatch 76700, batch loss 0.224789, batch nll 0.058353, batch error rate 4.000000%\n",
      "At minibatch 76800, batch loss 0.182139, batch nll 0.015748, batch error rate 0.000000%\n",
      "At minibatch 76900, batch loss 0.172996, batch nll 0.006649, batch error rate 0.000000%\n",
      "At minibatch 77000, batch loss 0.184122, batch nll 0.017828, batch error rate 0.000000%\n",
      "At minibatch 77100, batch loss 0.179334, batch nll 0.013086, batch error rate 0.000000%\n",
      "At minibatch 77200, batch loss 0.173177, batch nll 0.006976, batch error rate 0.000000%\n",
      "At minibatch 77300, batch loss 0.168447, batch nll 0.002293, batch error rate 0.000000%\n",
      "At minibatch 77400, batch loss 0.174718, batch nll 0.008614, batch error rate 0.000000%\n",
      "At minibatch 77500, batch loss 0.170816, batch nll 0.004759, batch error rate 0.000000%\n",
      "At minibatch 77600, batch loss 0.175001, batch nll 0.008993, batch error rate 0.000000%\n",
      "At minibatch 77700, batch loss 0.173514, batch nll 0.007554, batch error rate 0.000000%\n",
      "At minibatch 77800, batch loss 0.171006, batch nll 0.005099, batch error rate 0.000000%\n",
      "At minibatch 77900, batch loss 0.185538, batch nll 0.019682, batch error rate 0.000000%\n",
      "At minibatch 78000, batch loss 0.188854, batch nll 0.023046, batch error rate 0.000000%\n",
      "At minibatch 78100, batch loss 0.195212, batch nll 0.029446, batch error rate 0.000000%\n",
      "At minibatch 78200, batch loss 0.166517, batch nll 0.000799, batch error rate 0.000000%\n",
      "After epoch 40: valid_err_rate: 1.030000% currently going to do 40 epochs\n",
      "After epoch 40: averaged train_err_rate: 0.234117% averaged train nll: 0.014178 averaged train loss: 0.180360\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "while e<number_of_epochs: #This loop goes over epochs\n",
    "    e += 1\n",
    "    #First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in mnist_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 4e-3 * K / np.maximum(K, i)\n",
    "        momentum=0.9\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        #print [p.get_value().ravel()[:10] for p in model_parameters]\n",
    "        #print [p.get_value().ravel()[:10] for p in velocities]\n",
    "        \n",
    "        \n",
    "        train_loss.append((i,L))\n",
    "        train_erros.append((i,err_rate))\n",
    "        train_nll.append((i,nll))\n",
    "        if i % 100 == 0:\n",
    "            print(\"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100))\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(mnist_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i,val_error_rate))\n",
    "    print(\"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "        e, val_error_rate*100, number_of_epochs))\n",
    "    print(\"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "        e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "        np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "        np.mean(np.asarray(train_loss)[epoch_start_i:,1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 26\n",
      "Test error rate is 0.930000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff15a5fdba8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAECCAYAAAAVYxsVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff15b60ff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Setting network parameters from after epoch %d\" %(best_params_epoch))\n",
    "load_parameters(best_params)\n",
    "\n",
    "print(\"Test error rate is %f%%\" %(compute_error_rate(mnist_test_stream)*100.0,))\n",
    "\n",
    "subplot(2,1,1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:,0], train_nll_a[:,1], label='batch train nll')\n",
    "legend()\n",
    "\n",
    "subplot(2,1,2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:,0], train_erros_a[:,1], label='batch train error rate')\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:,0], validation_errors_a[:,1], label='validation error rate', color='r')\n",
    "ylim(0,0.2)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAACWCAYAAAAVI9lMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIUoboBClDVCI0gYo5D+U0oJ3+ZsDvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff158b7d240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#How do the filters in the first layer look like?\n",
    "\n",
    "plot_mat(CW1.get_value(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iii = predict.maker.inputs[0]\n",
    "X = iii.variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a function that shows how the network processes an image\n",
    "\n",
    "middle_layers_computer = theano.function([X], [\n",
    "        X,\n",
    "        after_C1,\n",
    "        after_P1,\n",
    "        after_C2,\n",
    "        after_P2\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEKCAYAAAAy4ujqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff158b10ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAACuCAYAAAD9ClyMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDER4UPbGGMi4v8AcxKgxQz5WSQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff15a0a0278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAACrCAYAAACtx80/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKkoTwkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff158be4ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAADmCAYAAAAeAdSNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff165ab9208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAADjCAYAAABOzEU+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wKxdB206tPb6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff165bb4cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_num=4\n",
    "\n",
    "middle_layers = middle_layers_computer(X_test_value[img_num:img_num+1])\n",
    "\n",
    "for ml, name in zip(middle_layers, ['X', 'C1', 'P1', 'C2', 'P2']):\n",
    "    plot_mat(ml.transpose(1,0,2,3), cmap='gray')\n",
    "    title(name)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
